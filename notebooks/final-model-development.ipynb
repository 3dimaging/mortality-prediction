{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.font_manager import FontProperties # for unicode fonts\n",
    "import sys\n",
    "import datetime as dt\n",
    "import mp_utils as mp\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "# used to print out pretty pandas dataframes\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# used to impute mean for data and standardize for computational stability\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# logistic regression is our favourite model ever\n",
    "from sklearn import linear_model\n",
    "from sklearn import ensemble\n",
    "\n",
    "# used to calculate AUROC/accuracy\n",
    "from sklearn import metrics\n",
    "\n",
    "# used to create confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# gradient boosting - must download package https://github.com/dmlc/xgboost\n",
    "import xgboost as xgb\n",
    "\n",
    "#from eli5 import show_weights\n",
    "\n",
    "# default colours for prettier plots\n",
    "col = [[0.9047, 0.1918, 0.1988],\n",
    "    [0.2941, 0.5447, 0.7494],\n",
    "    [0.3718, 0.7176, 0.3612],\n",
    "    [1.0000, 0.5482, 0.1000],\n",
    "    [0.4550, 0.4946, 0.4722],\n",
    "    [0.6859, 0.4035, 0.2412],\n",
    "    [0.9718, 0.5553, 0.7741],\n",
    "    [0.5313, 0.3359, 0.6523]];\n",
    "marker = ['v','o','d','^','s','o','+']\n",
    "ls = ['-','-','-','-','-','s','--','--']\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('X_design_matrix.csv').set_index('icustay_id')\n",
    "\n",
    "# create X by dropping idxK and the outcome\n",
    "y = df['death'].values\n",
    "idxK = df['idxK']\n",
    "X = df.drop(['death','idxK'],axis=1).values\n",
    "\n",
    "print('{} observations. Outcome rate: {:2.2f}%.'.format(X.shape[0],\n",
    "                                                        100.0*np.mean(y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "    colsample_bytree     - 0.7 \t[ 0.7 0.7 0.7 0.7 0.7 ]\n",
    "    silent               - 1.0 \t[ 1 1 1 1 1 ]\n",
    "    learning_rate        - 0.01 \t[ 0.01 0.01 0.01 0.01 0.01 ]\n",
    "    n_estimators         - 1000.0 \t[ 1000 1000 1000 1000 1000 ]\n",
    "    subsample            - 0.8 \t[ 0.8 0.8 0.8 0.8 0.8 ]\n",
    "    objective            - skipping as it is a string\n",
    "    max_depth            - 9.0 \t[ 9 6 9 9 6 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rough timing info:\n",
    "#     rf - 3 seconds per fold\n",
    "#    xgb - 30 seconds per fold\n",
    "# logreg - 4 seconds per fold\n",
    "#  lasso - 8 seconds per fold\n",
    "np.random.seed(7390984)\n",
    "\n",
    "# parameters from grid search\n",
    "xgb_mdl = xgb.XGBClassifier(colsample_bytree=0.7, silent=1,\n",
    "                            learning_rate = 0.01, n_estimators=1000,\n",
    "                            subsample=0.8, max_depth=9)\n",
    "\n",
    "models = {'xgb': xgb_mdl,\n",
    "          'lasso': linear_model.LassoCV(cv=5,fit_intercept=True,normalize=True,max_iter=10000),\n",
    "          'logreg': linear_model.LogisticRegression(fit_intercept=True),\n",
    "          'l2': linear_model.LogisticRegressionCV()\n",
    "          #'rf': ensemble.RandomForestClassifier()\n",
    "         }\n",
    "\n",
    "\n",
    "# create k-fold indices\n",
    "K = 5 # number of folds\n",
    "idxK = np.random.permutation(X.shape[0])\n",
    "idxK = np.mod(idxK,K)\n",
    "\n",
    "mdl_val = dict()\n",
    "results_val = dict()\n",
    "pred_val = dict()\n",
    "pred_val_merged = dict()\n",
    "for mdl in models:\n",
    "    print('=============== {} ==============='.format(mdl))\n",
    "    mdl_val[mdl] = list()\n",
    "    results_val[mdl] = list() # initialize list for scores\n",
    "    pred_val[mdl] = dict()\n",
    "    pred_val_merged[mdl] = np.zeros(X.shape[0])\n",
    "    \n",
    "    if mdl == 'xgb':\n",
    "        # no pre-processing of data necessary for xgb\n",
    "        estimator = Pipeline([(mdl, models[mdl])])\n",
    "\n",
    "    else:\n",
    "        estimator = Pipeline([(\"imputer\", Imputer(missing_values='NaN',\n",
    "                                          strategy=\"mean\",\n",
    "                                          axis=0)),\n",
    "                      (\"scaler\", StandardScaler()),\n",
    "                      (mdl, models[mdl])]) \n",
    "\n",
    "    for k in range(K):\n",
    "        # train the model using all but the kth fold\n",
    "        curr_mdl = sklearn.base.clone(estimator).fit(X[idxK != k, :], y[idxK != k])\n",
    "\n",
    "        # get prediction on this dataset\n",
    "        if mdl in ('lasso','ridge'):\n",
    "            curr_prob = curr_mdl.predict(X[idxK == k, :])\n",
    "        else:\n",
    "            curr_prob = curr_mdl.predict_proba(X[idxK == k, :])\n",
    "            curr_prob = curr_prob[:,1]\n",
    "            \n",
    "        pred_val_merged[mdl][idxK==k] = curr_prob\n",
    "        pred_val[mdl][k] = curr_prob\n",
    "\n",
    "        # calculate score (AUROC)\n",
    "        curr_score = metrics.roc_auc_score(y[idxK == k], curr_prob)\n",
    "\n",
    "        # add score to list of scores\n",
    "        results_val[mdl].append(curr_score)\n",
    "\n",
    "        # save the current model\n",
    "        mdl_val[mdl].append(curr_mdl)\n",
    "        \n",
    "        print('{} - Finished fold {} of {}. AUROC {:0.3f}.'.format(dt.datetime.now(), k+1, K, curr_score))\n",
    "    \n",
    "tar_val = dict()\n",
    "for k in range(K):\n",
    "    tar_val[k] = y[idxK==k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# average AUROC + min/max\n",
    "for mdl in models:\n",
    "    curr_score = np.zeros(K)\n",
    "    for k in range(K):\n",
    "        curr_score[k] = metrics.roc_auc_score(tar_val[k], pred_val[mdl][k])\n",
    "    print('{}\\t{:0.3f} [{:0.3f}, {:0.3f}]'.format(mdl, np.mean(curr_score), np.min(curr_score), np.max(curr_score)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average AUPRC + min/max\n",
    "for mdl in models:\n",
    "    curr_score = np.zeros(K)\n",
    "    for k in range(K):\n",
    "        curr_score[k] = metrics.average_precision_score(tar_val[k], pred_val[mdl][k])\n",
    "    print('{}\\t{:0.3f} [{:0.3f}, {:0.3f}]'.format(mdl, np.mean(curr_score), np.min(curr_score), np.max(curr_score)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot model performance closer to death"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in design matrices\n",
    "X_dict = {}\n",
    "idxK_dict = {}\n",
    "y_dict = {}\n",
    "\n",
    "for t in [0, 4, 8, 16, 24]:\n",
    "    df = pd.read_csv('X_design_matrix_{}hr_death.csv'.format(t)).set_index('icustay_id')\n",
    "\n",
    "    # create X by dropping idxK and the outcome\n",
    "    dm_string = 'Td={:02d}'.format(t)\n",
    "    y_dict[dm_string] = df['death'].values\n",
    "    idxK_dict[dm_string] = df['idxK']\n",
    "    X_dict[dm_string] = df.drop(['death','idxK'],axis=1).values\n",
    "\n",
    "    print('{:02d} - {} observations. Outcome rate: {:2.2f}%.'.format(t, X_dict[dm_string].shape[0],\n",
    "                                                            100.0*np.mean(y_dict[dm_string])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get predictions\n",
    "score_pred = dict()\n",
    "for m, mdl in enumerate(models):\n",
    "    all_score = list()\n",
    "    \n",
    "    score_pred[mdl] = dict()\n",
    "    for i, x in enumerate(xi_str):\n",
    "        curr_score = list()\n",
    "        for k in range(K):\n",
    "            curr_mdl = mdl_val[mdl][k]\n",
    "\n",
    "            # get prediction on this dataset\n",
    "            if mdl in ('lasso','ridge'):\n",
    "                curr_prob = curr_mdl.predict(X_dict[x][idxK_dict[x] == k, :])\n",
    "            else:\n",
    "                curr_prob = curr_mdl.predict_proba(X_dict[x][idxK_dict[x] == k, :])\n",
    "                curr_prob = curr_prob[:,1]\n",
    "\n",
    "            # calculate score (AUROC)\n",
    "            curr_score.append(metrics.roc_auc_score(y_dict[x][idxK_dict[x] == k], curr_prob))\n",
    "        score_pred[mdl][x] = curr_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a figure of the results\n",
    "marker = ['o','s','v','d']\n",
    "xi_str = ['Td=00','Td=04','Td=08','Td=16','Td=24']\n",
    "xi = [int(x[-2:]) for x in xi_str]\n",
    "\n",
    "mdl_dict = {'logreg': 'LR', 'xgb': 'GB', 'l2': 'L2', 'lasso': 'LASSO'}\n",
    "plt.figure(figsize=[14,10])\n",
    "for m, mdl in enumerate(models):\n",
    "    all_score = list()\n",
    "    for i, x in enumerate(xi_str):\n",
    "        curr_score = score_pred[mdl][x]\n",
    "        plt.plot(xi[i] * np.ones(len(curr_score)), curr_score,\n",
    "                marker=marker[m], color=col[m],\n",
    "                markersize=10, linewidth=2, linestyle=':')\n",
    "\n",
    "        all_score.append(np.median(curr_score))\n",
    "        \n",
    "    # plot a line through the mean across all evaluations\n",
    "    plt.plot(xi, all_score,\n",
    "            marker=marker[m], color=col[m],\n",
    "            markersize=10, linewidth=2, linestyle='-',\n",
    "            label=mdl_dict[mdl])\n",
    "\n",
    "plt.gca().set_xticks(np.linspace(0,24,7))\n",
    "plt.gca().set_xlim([-1,25])\n",
    "plt.gca().invert_xaxis()\n",
    "plt.legend(loc='lower center',fontsize=16)\n",
    "plt.xlabel('Lead time (hours)',fontsize=18)\n",
    "plt.ylabel('AUROC',fontsize=18)\n",
    "\n",
    "ax = plt.gca()\n",
    "\n",
    "for tick in ax.xaxis.get_major_ticks():\n",
    "    tick.label.set_fontsize(16) \n",
    "for tick in ax.yaxis.get_major_ticks():\n",
    "    tick.label.set_fontsize(16) \n",
    "\n",
    "plt.grid()\n",
    "plt.savefig('experimentb_auroc_over_time.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
