{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.font_manager import FontProperties # for unicode fonts\n",
    "import psycopg2\n",
    "import sys\n",
    "import datetime as dt\n",
    "import mp_utils as mp\n",
    "\n",
    "import sklearn\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# used for train/test splits and cross validation\n",
    "from sklearn import model_selection\n",
    "\n",
    "# used to impute mean for data and standardize for computational stability\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# logistic regression is our favourite model ever\n",
    "from sklearn import linear_model\n",
    "from sklearn import ensemble\n",
    "\n",
    "# used to calculate AUROC/accuracy\n",
    "from sklearn import metrics\n",
    "\n",
    "# used to create confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# gradient boosting - must download package https://github.com/dmlc/xgboost\n",
    "import xgboost as xgb\n",
    "\n",
    "# default colours for prettier plots\n",
    "col = [[0.9047, 0.1918, 0.1988],\n",
    "    [0.2941, 0.5447, 0.7494],\n",
    "    [0.3718, 0.7176, 0.3612],\n",
    "    [1.0000, 0.5482, 0.1000],\n",
    "    [0.4550, 0.4946, 0.4722],\n",
    "    [0.6859, 0.4035, 0.2412],\n",
    "    [0.9718, 0.5553, 0.7741],\n",
    "    [0.5313, 0.3359, 0.6523]];\n",
    "marker = ['v','o','d','^','s','o','+']\n",
    "ls = ['-','-','-','-','-','s','--','--']\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook outline\n",
    "\n",
    "This notebook will evaluate the mortality prediction model during the first 24 hours of the patient's ICU stay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in design matrix\n",
    "df = pd.read_csv('X_design_matrix_24hr.csv').set_index('icustay_id')\n",
    "\n",
    "# create X by dropping idxK and the outcome\n",
    "y = df['death'].values\n",
    "idxK = df['idxK']\n",
    "K = np.max(idxK)+1\n",
    "X = df.drop(['death','idxK'],axis=1).values\n",
    "\n",
    "print('{} observations. Outcome rate: {:2.2f}%.'.format(X.shape[0],\n",
    "                                                        100.0*np.mean(y)))\n",
    "\n",
    "\n",
    "# load in static data which includes severity of illness scores\n",
    "df_static = pd.read_csv('df_static_data.csv').set_index('icustay_id')\n",
    "df_other = pd.read_csv('df_soi.csv').set_index('icustay_id')\n",
    "\n",
    "# join static to icustay_id\n",
    "df_other = df_other.loc[df.index, :]\n",
    "df_static = df_static.loc[df.index, :]\n",
    "print('{} observations in df_other.'.format(df_other.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate severity of illness scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_other.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average AUROC + min/max\n",
    "for mdl in ['saps','sapsii','apsiii','sofa','lods','oasis']:\n",
    "    curr_score = list()\n",
    "    for k in range(K):\n",
    "        auc = metrics.roc_auc_score(y[idxK==k], df_other.loc[idxK==k,mdl])\n",
    "        curr_score.append(auc)\n",
    "    print('{}\\t{:0.3f} [{:0.3f}, {:0.3f}]'.format(mdl, np.mean(curr_score), np.min(curr_score), np.max(curr_score)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# average AUPRC + min/max\n",
    "for mdl in ['saps','sapsii','apsiii','sofa','lods','oasis']:\n",
    "    curr_score = list()\n",
    "    for k in range(K):\n",
    "        auc = metrics.average_precision_score(y[idxK==k], df_other.loc[idxK==k,mdl])\n",
    "        curr_score.append(auc)\n",
    "    print('{}\\t{:0.3f} [{:0.3f}, {:0.3f}]'.format(\n",
    "        mdl, np.mean(curr_score), np.min(curr_score), np.max(curr_score)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: Using first 24 hours of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we loop through all the design matrices and get an idea of the CV performance of each.\n",
    "\n",
    "Here are some additional models worth considering:\n",
    "\n",
    "```python\n",
    "models = {'l2logreg': LogisticRegressionCV(penalty='l2',cv=5,fit_intercept=True),\n",
    "     'lasso': LassoCV(cv=5,fit_intercept=True),\n",
    "     'xgb': xgb.XGBClassifier(max_depth=3, n_estimators=300, learning_rate=0.05),\n",
    "     'logreg': LogisticRegression(fit_intercept=True)}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rough timing info:\n",
    "#     rf - 3 seconds per fold\n",
    "#    xgb - 30 seconds per fold\n",
    "# logreg - 4 seconds per fold\n",
    "#  lasso - 8 seconds per fold\n",
    "np.random.seed(73912984)\n",
    "\n",
    "# parameters from grid search\n",
    "xgb_mdl = xgb.XGBClassifier(colsample_bytree=0.7, silent=1,\n",
    "                            learning_rate = 0.01, n_estimators=1000,\n",
    "                            subsample=0.8, max_depth=9)\n",
    "\n",
    "models = {'xgb': xgb_mdl,\n",
    "          'lasso': linear_model.LassoCV(cv=5,fit_intercept=True,normalize=True,max_iter=10000),\n",
    "          'logreg': linear_model.LogisticRegression(fit_intercept=True),\n",
    "          'l2': linear_model.LogisticRegressionCV()\n",
    "          #'rf': ensemble.RandomForestClassifier()\n",
    "         }\n",
    "\n",
    "\n",
    "# create k-fold indices\n",
    "K = 5 # number of folds\n",
    "idxK = np.random.permutation(X.shape[0])\n",
    "idxK = np.mod(idxK,K)\n",
    "\n",
    "mdl_val = dict()\n",
    "results_val = dict()\n",
    "pred_val = dict()\n",
    "pred_val_merged = dict()\n",
    "for mdl in models:\n",
    "    print('=============== {} ==============='.format(mdl))\n",
    "    mdl_val[mdl] = list()\n",
    "    results_val[mdl] = list() # initialize list for scores\n",
    "    pred_val[mdl] = dict()\n",
    "    pred_val_merged[mdl] = np.zeros(X.shape[0])\n",
    "    \n",
    "    if mdl == 'xgb':\n",
    "        # no pre-processing of data necessary for xgb\n",
    "        estimator = Pipeline([(mdl, models[mdl])])\n",
    "\n",
    "    else:\n",
    "        estimator = Pipeline([(\"imputer\", Imputer(missing_values='NaN',\n",
    "                                          strategy=\"mean\",\n",
    "                                          axis=0)),\n",
    "                      (\"scaler\", StandardScaler()),\n",
    "                      (mdl, models[mdl])]) \n",
    "\n",
    "    for k in range(K):\n",
    "        # train the model using all but the kth fold\n",
    "        curr_mdl = sklearn.base.clone(estimator).fit(X[idxK != k, :], y[idxK != k])\n",
    "\n",
    "        # get prediction on this dataset\n",
    "        if mdl in ('lasso','ridge'):\n",
    "            curr_prob = curr_mdl.predict(X[idxK == k, :])\n",
    "        else:\n",
    "            curr_prob = curr_mdl.predict_proba(X[idxK == k, :])\n",
    "            curr_prob = curr_prob[:,1]\n",
    "            \n",
    "        pred_val_merged[mdl][idxK==k] = curr_prob\n",
    "        pred_val[mdl][k] = curr_prob\n",
    "\n",
    "        # calculate score (AUROC)\n",
    "        curr_score = metrics.roc_auc_score(y[idxK == k], curr_prob)\n",
    "\n",
    "        # add score to list of scores\n",
    "        results_val[mdl].append(curr_score)\n",
    "\n",
    "        # save the current model\n",
    "        mdl_val[mdl].append(curr_mdl)\n",
    "        \n",
    "        print('{} - Finished fold {} of {}. AUROC {:0.3f}.'.format(dt.datetime.now(), k+1, K, curr_score))\n",
    "    \n",
    "tar_val = dict()\n",
    "for k in range(K):\n",
    "    tar_val[k] = y[idxK==k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average AUROC + min/max\n",
    "for mdl in models:\n",
    "    curr_score = np.zeros(K)\n",
    "    for k in range(K):\n",
    "        curr_score[k] = metrics.roc_auc_score(tar_val[k], pred_val[mdl][k])\n",
    "    print('{}\\t{:0.3f} [{:0.3f}, {:0.3f}]'.format(mdl, np.mean(curr_score), np.min(curr_score), np.max(curr_score)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average AUPRC + min/max\n",
    "for mdl in models:\n",
    "    curr_score = np.zeros(K)\n",
    "    for k in range(K):\n",
    "        curr_score[k] = metrics.average_precision_score(tar_val[k], pred_val[mdl][k])\n",
    "    print('{}\\t{:0.3f} [{:0.3f}, {:0.3f}]'.format(mdl, np.mean(curr_score), np.min(curr_score), np.max(curr_score)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_static.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in [u'is_male', u'service_med',\n",
    "       u'service_cmed', u'service_omed', u'service_nmed', u'service_nsurg',\n",
    "       u'service_tsurg', u'service_csurg', u'service_vsurg', u'service_ortho',\n",
    "       u'service_psurg', u'service_surg', u'service_gu', u'service_gyn',\n",
    "       u'service_traum', u'service_ent', u'service_any_noncard_surg',\n",
    "       u'service_any_card_surg', u'age', u'race_black', u'race_hispanic',\n",
    "       u'race_asian', u'race_other', u'emergency_admission', u'height',\n",
    "       u'weight', u'bmi']:\n",
    "    if 'str' not in str(type(df_static[c])):\n",
    "        print('{} {} {}'.format(c, df_static[c].mean(), df_static[c].std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#analyses = ['base', 'base_nodeathfix', '00', '04', '08','16',\n",
    "#            '24','fixed', 'wt8', 'wt16', 'wt24',\n",
    "#            'wt8_00', 'wt8_08', 'wt8_16', 'wt8_24']\n",
    "\n",
    "seeds = {'base': 473010,\n",
    "        'base_nodeathfix': 217632,\n",
    "        '00': 724311,\n",
    "        '04': 952227,\n",
    "        '08': 721297,\n",
    "        '16': 968879,\n",
    "        '24': 608972,\n",
    "        'fixed': 585794,\n",
    "        'wt8': 176381,\n",
    "        'wt16': 658229,\n",
    "        'wt24': 635170,\n",
    "        'wt8_00': 34741,\n",
    "        'wt8_08': 95467,\n",
    "        'wt8_16': 85349,\n",
    "        'wt8_24': 89642,\n",
    "        'wt24_fixed': 761456}\n",
    "\n",
    "data_ext = 'wt24_fixed'\n",
    "\n",
    "# SVM parameters tuned by cross-validation\n",
    "#svm_parameters = {'kernel': ['rbf'], 'gamma': [1e-3, 1e-4],\n",
    "#                     'C': [1, 10]}\n",
    "\n",
    "# use a full grid over all parameters\n",
    "# specify parameters and distributions to sample from\n",
    "N_FEAT = X.shape[1]\n",
    "param_dist = {\"max_depth\": [3, 7, None],\n",
    "              \"max_features\": sp.stats.randint(1, N_FEAT),\n",
    "              \"min_samples_split\": sp.stats.randint(1, N_FEAT),\n",
    "              \"min_samples_leaf\": sp.stats.randint(1, N_FEAT),\n",
    "              \"n_estimators\": sp.stats.randint(50, 500),\n",
    "              \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "# set up randomized search for RF\n",
    "n_iter_search = 20\n",
    "rf_random_search = RandomizedSearchCV(sklearn.ensemble.RandomForestClassifier(),\n",
    "                                      param_distributions=param_dist,\n",
    "                                      n_iter=n_iter_search)\n",
    "\n",
    "\n",
    "models = {'xgb': xgb.XGBClassifier(max_depth=3, n_estimators=300, learning_rate=0.05),\n",
    "          'lasso': LassoCV(cv=5,fit_intercept=True),\n",
    "          'logreg': LogisticRegression(fit_intercept=True),\n",
    "          'rf': sklearn.ensemble.RandomForestClassifier(),\n",
    "          #'svm': GridSearchCV(sklearn.svm.SVC(kernel='rbf',class_weight='balanced',probability=False),\n",
    "          #                   svm_parameters, cv=5, scoring='roc_auc')\n",
    "         }\n",
    "\n",
    "results = dict()\n",
    "\n",
    "np.random.seed(seed=seeds[data_ext])\n",
    "\n",
    "# load the data into a numpy array\n",
    "X, y, X_header = mp.load_design_matrix(co,\n",
    "                                       df_additional_data=df_static[vars_static],\n",
    "                                       data_ext='_' + data_ext)\n",
    "\n",
    "print('{} - ========= {} ========='.format(dt.datetime.now(), data_ext))\n",
    "\n",
    "scores = list()\n",
    "for i, mdl in enumerate(models):\n",
    "    if mdl == 'xgb':\n",
    "        # no pre-processing of data necessary for xgb\n",
    "        estimator = Pipeline([(mdl, models[mdl])])\n",
    "\n",
    "    else:\n",
    "        estimator = Pipeline([(\"imputer\", Imputer(missing_values='NaN',\n",
    "                                          strategy=\"mean\",\n",
    "                                          axis=0)),\n",
    "                      (\"scaler\", StandardScaler()),\n",
    "                      (mdl, models[mdl])])\n",
    "\n",
    "\n",
    "    curr_score = cross_val_score(estimator, X, y, scoring='roc_auc',cv=5)\n",
    "\n",
    "    print('{} - {:10s} {:0.4f} [{:0.4f}, {:0.4f}]'.format(dt.datetime.now(), mdl,\n",
    "                                                          np.mean(curr_score),\n",
    "                                                          np.min(curr_score), np.max(curr_score)))\n",
    "\n",
    "    # save the score to a dictionary\n",
    "    results[mdl] = curr_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compare to severity of illness scores\n",
    "df = co\n",
    "\n",
    "# merge in the various severity scores\n",
    "df = df.merge(oa, how='left', left_index=True,right_index=True,suffixes=('','_oasis'))\n",
    "df = df.merge(sofa, how='left', left_index=True,right_index=True,suffixes=('','_sofa'))\n",
    "df = df.merge(saps, how='left', left_index=True,right_index=True,suffixes=('','_saps'))\n",
    "df = df.merge(sapsii, how='left', left_index=True,right_index=True,suffixes=('','_sapsii'))\n",
    "df = df.merge(apsiii, how='left', left_index=True,right_index=True,suffixes=('','_apsiii'))\n",
    "\n",
    "\n",
    "for v in df.columns:\n",
    "    if v != 'hospital_expire_flag':\n",
    "        print('{:8s} - {:0.4f}'.format(v,metrics.roc_auc_score(df['hospital_expire_flag'],df[v])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print the results\n",
    "mdl = 'xgb'\n",
    "\n",
    "print('=================== {} ==================='.format(mdl))\n",
    "\n",
    "for data_ext in np.sort(results_val.keys()):\n",
    "    curr_score = results[mdl][data_ext]\n",
    "    print('{:15s} - {:0.4f} [{:0.4f} - {:0.4f}]'.format(data_ext, np.mean(curr_score), np.min(curr_score), np.max(curr_score)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above reported cross-validation performance in a variety of settings. We're also interested in *evaluating* the same model in the various settings. That is, training a model using random offsets, and then evaluating how it performs 4 hours before death, 8 hours, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# extract the data used to train the model\n",
    "data_ext = 'base'\n",
    "np.random.seed(seed=seeds[data_ext])\n",
    "\n",
    "# load the data into a numpy array\n",
    "X, y, X_header = mp.load_design_matrix(co,\n",
    "                                       df_additional_data=df_static[vars_static],\n",
    "                                       data_ext=data_ext)\n",
    "\n",
    "\n",
    "    \n",
    "# load into a dictionary the other various datasets/models\n",
    "X_val = dict()\n",
    "y_val = dict()\n",
    "X_header_val = dict()\n",
    "results_val = dict() # stores AUROCs across datasets\n",
    "mdl_val = dict() # stores the model trained across k-folds\n",
    "\n",
    "for i, data_ext in enumerate(analyses):\n",
    "\n",
    "    # load the data into a numpy array\n",
    "    X_val[data_ext], y_val[data_ext], X_header_val[data_ext] = mp.load_design_matrix(co,\n",
    "                                           df_additional_data=df_static[vars_static],\n",
    "                                           data_ext=data_ext)\n",
    "    results_val[data_ext] = dict()\n",
    "    \n",
    "print('{} - Finished loading data'.format(dt.datetime.now()))\n",
    "\n",
    "np.random.seed(seed=seeds[data_ext])\n",
    "\n",
    "# create k-fold indices\n",
    "K = 5 # number of folds\n",
    "idxK = np.random.permutation(X.shape[0])\n",
    "idxK = np.mod(idxK,K)\n",
    "\n",
    "mdl = 'xgb'\n",
    "mdl_val[mdl] = list()\n",
    "\n",
    "\n",
    "for data_ext in X_val:\n",
    "    results_val[data_ext][mdl] = list() # initialize list for scores\n",
    "\n",
    "# no pre-processing of data necessary for xgb\n",
    "estimator = Pipeline([(mdl, models[mdl])])    \n",
    "\n",
    "for k in range(K):\n",
    "    # train the model using all but the kth fold\n",
    "    curr_mdl = estimator.fit(X[idxK != k, :],y[idxK != k])\n",
    "\n",
    "    for data_ext in X_val:\n",
    "        # get prediction on this dataset\n",
    "        curr_prob = curr_mdl.predict_proba(X_val[data_ext][idxK == k, :])\n",
    "        curr_prob = curr_prob[:,1]\n",
    "\n",
    "        # calculate score (AUROC)\n",
    "        curr_score = metrics.roc_auc_score(y_val[data_ext][idxK == k], curr_prob)\n",
    "\n",
    "        # add score to list of scores\n",
    "        results_val[data_ext][mdl].append(curr_score)\n",
    "\n",
    "        # save the current model\n",
    "        mdl_val[mdl].append(curr_mdl)\n",
    "    \n",
    "    print('{} - Finished fold {} of {}.'.format(dt.datetime.now(), k+1, K))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print the results\n",
    "mdl = 'xgb'\n",
    "\n",
    "print('=================== {} ==================='.format(mdl))\n",
    "\n",
    "for data_ext in np.sort(results_val.keys()):\n",
    "    curr_score = results_val[data_ext][mdl]\n",
    "    print('{:15s} - {:0.4f} [{:0.4f} - {:0.4f}]'.format(data_ext, np.mean(curr_score), np.min(curr_score), np.max(curr_score)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat the same experiment as above, but this time, let's train a model with the outcome \"did the patient die in the next 24 hours?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# extract the data\n",
    "np.random.seed(seed=seeds[data_ext])\n",
    "data_ext = 'base'\n",
    "\n",
    "# load the data into a numpy array\n",
    "X, y, X_header = mp.load_design_matrix(co,\n",
    "                                       df_additional_data=df_static[vars_static],\n",
    "                                       data_ext=data_ext,\n",
    "                                       diedWithin=24)\n",
    "\n",
    "# load into a dictionary the other various datasets/models\n",
    "X_val = dict()\n",
    "y_val = dict()\n",
    "X_header_val = dict()\n",
    "results_val = dict() # stores AUROCs across datasets\n",
    "mdl_val = dict() # stores the model trained across k-folds\n",
    "\n",
    "for i, data_ext in enumerate(analyses):\n",
    "\n",
    "    # load the data into a numpy array\n",
    "    X_val[data_ext], y_val[data_ext], X_header_val[data_ext] = mp.load_design_matrix(co,\n",
    "                                           df_additional_data=df_static[vars_static],\n",
    "                                           data_ext='_' + data_ext)\n",
    "    results_val[data_ext] = dict()\n",
    "    \n",
    "print('{} - Finished loading data'.format(dt.datetime.now()))\n",
    "\n",
    "np.random.seed(seed=seeds[data_ext])\n",
    "\n",
    "# create k-fold indices\n",
    "K = 5 # number of folds\n",
    "idxK = np.random.permutation(X.shape[0])\n",
    "idxK = np.mod(idxK,K)\n",
    "\n",
    "mdl = 'xgb'\n",
    "mdl_val[mdl] = list()\n",
    "\n",
    "\n",
    "for data_ext in X_val:\n",
    "    results_val[data_ext][mdl] = list() # initialize list for scores\n",
    "\n",
    "# no pre-processing of data necessary for xgb\n",
    "estimator = Pipeline([(mdl, models[mdl])])    \n",
    "\n",
    "for k in range(K):\n",
    "    # train the model using all but the kth fold\n",
    "    curr_mdl = estimator.fit(X[idxK != k, :],y[idxK != k])\n",
    "\n",
    "    for data_ext in X_val:\n",
    "        # get prediction on this dataset\n",
    "        curr_prob = curr_mdl.predict_proba(X_val[data_ext][idxK == k, :])\n",
    "        curr_prob = curr_prob[:,1]\n",
    "\n",
    "        # calculate score (AUROC)\n",
    "        curr_score = metrics.roc_auc_score(y_val[data_ext][idxK == k], curr_prob)\n",
    "\n",
    "        # add score to list of scores\n",
    "        results_val[data_ext][mdl].append(curr_score)\n",
    "\n",
    "        # save the current model\n",
    "        mdl_val[mdl].append(curr_mdl)\n",
    "    \n",
    "    print('{} - Finished fold {} of {}.'.format(dt.datetime.now(), k+1, K))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print the results\n",
    "mdl = 'xgb'\n",
    "\n",
    "print('=================== {} ==================='.format(mdl))\n",
    "\n",
    "for data_ext in np.sort(results_val.keys()):\n",
    "    curr_score = results_val[data_ext][mdl]\n",
    "    print('{:15s} - {:0.4f} [{:0.4f} - {:0.4f}]'.format(data_ext, np.mean(curr_score), np.min(curr_score), np.max(curr_score)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have an estimate of how well these models do in cross-validation. The next step will be to take the best model and optimize it appropriately using only a training subset of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create training / test sets\n",
    "np.random.seed(seed=324875)\n",
    "icustay_id = co.index.values\n",
    "idxTest = np.random.rand(X.shape[0]) > 0.20\n",
    "X_train = X[~idxTest,:]\n",
    "y_train = y[~idxTest]\n",
    "iid_train = icustay_id[~idxTest]\n",
    "\n",
    "X_test = X[idxTest,:]\n",
    "y_test = y[idxTest]\n",
    "iid_test = icustay_id[~idxTest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# optimize hyperparameters of a model using only the training set\n",
    "# takes ~20 minutes\n",
    "\n",
    "# first train it w/o grid search\n",
    "xgb_nopreproc = xgb.XGBClassifier(max_depth=3, n_estimators=300, learning_rate=0.05)\n",
    "xgb_nopreproc = xgb_nopreproc.fit(X_train, y_train)\n",
    "\n",
    "# parameters with multiple values will be used in the grid search\n",
    "grid_params = {\n",
    "         'max_depth': [4,7], # max depth of the tree\n",
    "         'learning_rate': [0.05, 0.3], # step size shrinkage, makes earlier trees less important over time\n",
    "         'n_estimators': [300, 1000], # number of trees built\n",
    "         'subsample': [0.3, 0.8] # subsample the data when fitting each tree (prevent overfitting)\n",
    "         }\n",
    "\n",
    "default_params = {'colsample_bytree': 1,\n",
    "                  'colsample_bylevel':1,\n",
    "                  'silent':1,\n",
    "                  'reg_lambda':1, # L2 regularization on weights\n",
    "                  'reg_alpha':0, # L1 regularization on weights\n",
    "                  'objective':'binary:logistic'}\n",
    "\n",
    "init_model = xgb.XGBClassifier(**default_params)\n",
    "\n",
    "# the pipeline here is redundant - but could be useful if you want to add any custom preprocessing\n",
    "# for example, creating binary features from categories, etc...\n",
    "# the custom function only has to implement 'fit' and 'transform'\n",
    "estimator = Pipeline([(\"xgb\", GridSearchCV(init_model, grid_params, verbose=1))])\n",
    "\n",
    "xgb_model_cv = estimator.fit(X_train,y_train)\n",
    "\n",
    "# generate class probabilities\n",
    "y_prob = xgb_model_cv.predict_proba(X_test)\n",
    "y_prob = y_prob[:, 1]\n",
    "\n",
    "# predict class labels for the test set\n",
    "y_pred = (y_prob > 0.5).astype(int)\n",
    "\n",
    "# get the original xgb predictions without cross-validation\n",
    "# gives us a rough idea of the improvement of selecting some of the parameters\n",
    "y_prob_nocv = xgb_nopreproc.predict_proba(X_test)[:,1]\n",
    "\n",
    "print('\\n --- Performance on 20% held out test set --- \\n')\n",
    "# generate evaluation metrics\n",
    "print('Accuracy = {:0.3f}'.format(metrics.accuracy_score(y_test, y_pred)))            \n",
    "print('AUROC = {:0.3f} (unoptimized model was {:0.3f})'.format(metrics.roc_auc_score(y_test, y_prob),\n",
    "                                                               metrics.roc_auc_score(y_test, y_prob_nocv)))\n",
    "\n",
    "\n",
    "mp.print_cm(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the above optimized hyperparameters, train the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#best_params = xgb_model_cv.get_params()['xgb'].best_params_\n",
    "xgb_model = xgb.XGBClassifier(**default_params)\n",
    "#xgb_model = xgb_model.set_params(**best_params)\n",
    "xgb_model = xgb_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# feature importance!\n",
    "plt.figure(figsize=[14,40])\n",
    "ax = plt.gca()\n",
    "mp.plot_xgb_importance_fmap(xgb_model, X_header=X_header, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM is just too slow :(\n",
    "\n",
    "```python\n",
    "# speed up SVM\n",
    "\n",
    "estimator = Pipeline([(\"imputer\", Imputer(missing_values='NaN',\n",
    "                                  strategy=\"mean\",\n",
    "                                  axis=0)),\n",
    "              (\"scaler\", sklearn.preprocessing.MinMaxScaler()),\n",
    "              (\"svm\", sklearn.svm.SVC(cache_size=6000))])\n",
    "\n",
    "for n in [100,1000,10000]:\n",
    "    print(n)\n",
    "    %timeit estimator.fit(X[0:n,:],y[0:n])\n",
    "```\n",
    "\n",
    "~10,000 samples take ~5s and that's using default parameters.\n",
    "\n",
    "```python\n",
    "# speed up SVM with bagging\n",
    "n_estimators = 10\n",
    "estimator = Pipeline([(\"imputer\", Imputer(missing_values='NaN',\n",
    "                                  strategy=\"mean\",\n",
    "                                  axis=0)),\n",
    "              (\"scaler\", sklearn.preprocessing.MinMaxScaler()),\n",
    "              (\"svm_bagged\", BaggingClassifier(sklearn.svm.SVC(kernel='linear',\n",
    "                                                  probability=False,\n",
    "                                                  class_weight='balanced',\n",
    "                                                  cache_size=6000), \n",
    "                                               max_samples = 1.0 / n_estimators,\n",
    "                                               n_estimators=n_estimators,\n",
    "                                               bootstrap=False))])\n",
    "\n",
    "for n in [100,1000,10000]:\n",
    "    print(n)\n",
    "    %timeit estimator.fit(X[0:n,:],y[0:n])\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
