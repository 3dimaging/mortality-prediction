{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import psycopg2\n",
    "import sys\n",
    "import datetime as dt\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# used for train/test splits and cross validation\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# used to impute mean for data and standardize for computational stability\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# logistic regression is our favourite model ever\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV # l2 regularized regression\n",
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "# used to calculate AUROC/accuracy\n",
    "from sklearn import metrics\n",
    "\n",
    "# used to create confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "# gradient boosting - must download package https://github.com/dmlc/xgboost\n",
    "import xgboost as xgb\n",
    "\n",
    "# default colours for prettier plots\n",
    "\n",
    "col = [[0.9047, 0.1918, 0.1988],\n",
    "    [0.2941, 0.5447, 0.7494],\n",
    "    [0.3718, 0.7176, 0.3612],\n",
    "    [1.0000, 0.5482, 0.1000],\n",
    "    [0.4550, 0.4946, 0.4722],\n",
    "    [0.6859, 0.4035, 0.2412],\n",
    "    [0.9718, 0.5553, 0.7741],\n",
    "    [0.5313, 0.3359, 0.6523]];\n",
    "marker = ['v','o','d','^','s','o','+']\n",
    "ls = ['-','-','-','-','-','s','--','--']\n",
    "\n",
    "# pretty confusion matrices!\n",
    "def print_cm(y, yhat):\n",
    "    print('\\nConfusion matrix')\n",
    "    cm = metrics.confusion_matrix(y, yhat)\n",
    "    TN = cm[0,0]\n",
    "    FP = cm[0,1]\n",
    "    FN = cm[1,0]\n",
    "    TP = cm[1,1]\n",
    "    N = TN+FP+FN+TP\n",
    "    print('   \\t{:6s}\\t{:6s}').format('yhat=0','yhat=1')\n",
    "    print('y=0\\t{:6g}\\t{:6g}\\tNPV={:2.2f}').format(cm[0,0],cm[0,1], 100.0*TN / (TN+FN)) # NPV\n",
    "    print('y=1\\t{:6g}\\t{:6g}\\tPPV={:2.2f}').format(cm[1,0],cm[1,1], 100.0*TP / (TP+FP)) # PPV\n",
    "    # add sensitivity/specificity as the bottom line\n",
    "    print('   \\t{:2.2f}\\t{:2.2f}\\tAcc={:2.2f}').format(100.0*TN/(TN+FP), 100.0*TP/(TP+FN), 100.0*(TP+TN)/N)\n",
    "    print('   \\tSpec\\tSens')\n",
    "    \n",
    "    \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# below config used on pc70\n",
    "sqluser = 'alistairewj'\n",
    "dbname = 'mimic'\n",
    "schema_name = 'mimiciii'\n",
    "\n",
    "# Connect to local postgres version of mimic\n",
    "con = psycopg2.connect(dbname=dbname, user=sqluser)\n",
    "cur = con.cursor()\n",
    "cur.execute('SET search_path to ' + schema_name)\n",
    "\n",
    "# extract data from each materialized view\n",
    "mp_bloodgasarterial = pd.read_sql_query(\"select * from mp_bloodgasarterial\",con)\n",
    "mp_gcs = pd.read_sql_query(\"select * from mp_gcs\",con)\n",
    "mp_height = pd.read_sql_query(\"select * from mp_height\",con)\n",
    "mp_labs = pd.read_sql_query(\"select * from mp_labs\",con)\n",
    "mp_rass = pd.read_sql_query(\"select * from mp_rass\",con)\n",
    "mp_rrt = pd.read_sql_query(\"select * from mp_rrt\",con)\n",
    "#mp_service = pd.read_sql_query(\"select * from mp_service\",con)\n",
    "mp_uo = pd.read_sql_query(\"select * from mp_uo\",con)\n",
    "mp_vasopressor = pd.read_sql_query(\"select * from mp_vasopressor\",con)\n",
    "mp_vent = pd.read_sql_query(\"select * from mp_vent\",con)\n",
    "mp_vitals = pd.read_sql_query(\"select * from mp_vitals\",con)\n",
    "mp_weight = pd.read_sql_query(\"select * from mp_weight\",con)\n",
    "\n",
    "# define our cohort\n",
    "# exclusion criteria:\n",
    "#   - less than 16 years old\n",
    "#   - stayed in the ICU less than 4 hours\n",
    "#   - never have any chartevents data (i.e. likely administrative error)\n",
    "query = \\\n",
    "\"\"\"\n",
    "with t1 as\n",
    "(\n",
    "select ie.icustay_id\n",
    "    , adm.HOSPITAL_EXPIRE_FLAG\n",
    "    , ROW_NUMBER() over (partition by ie.subject_id order by intime) as rn\n",
    "from icustays ie\n",
    "inner join admissions adm\n",
    "    on ie.hadm_id = adm.hadm_id\n",
    "inner join patients pat\n",
    "    on ie.subject_id = pat.subject_id\n",
    "    and pat.dob < ie.intime - interval '16' year\n",
    "where adm.HAS_CHARTEVENTS_DATA = 1\n",
    "and (ie.outtime - ie.intime) >= interval '4' hour\n",
    ")\n",
    "select \n",
    "    icustay_id\n",
    "    , HOSPITAL_EXPIRE_FLAG\n",
    "from t1\n",
    "\"\"\"\n",
    "co = pd.read_sql_query(query,con)\n",
    "\n",
    "#TODO: delete subject_id // hadm_id from one of the data tables\n",
    "\n",
    "# merge in the data into co from the various other tables\n",
    "# note we start off with co then merge into the created dataframe\n",
    "df = co.merge(mp_bloodgasarterial, how='inner', on='icustay_id',suffixes=('','_bloodgasarterial'))\n",
    "df = df.merge(mp_gcs, how='inner', on='icustay_id',suffixes=('','_gcs'))\n",
    "df = df.merge(mp_height, how='inner', on='icustay_id',suffixes=('','_height'))\n",
    "df = df.merge(mp_labs, how='inner', on='icustay_id',suffixes=('','_labs'))\n",
    "df = df.merge(mp_rass, how='inner', on='icustay_id',suffixes=('','_rass'))\n",
    "df = df.merge(mp_rrt, how='inner', on='icustay_id',suffixes=('','_rrt'))\n",
    "df = df.merge(mp_uo, how='inner', on='icustay_id',suffixes=('','_uo'))\n",
    "df = df.merge(mp_vasopressor, how='inner', on='icustay_id',suffixes=('','_vasopressor'))\n",
    "df = df.merge(mp_vent, how='inner', on='icustay_id',suffixes=('','_vent'))\n",
    "df = df.merge(mp_vitals, how='inner', on='icustay_id',suffixes=('','_vitals'))\n",
    "df = df.merge(mp_weight, how='inner', on='icustay_id',suffixes=('','_weight'))\n",
    "\n",
    "# also get some static vars to compare model w/ and w/o them\n",
    "mp_static_vars = pd.read_sql_query(\"select * from mp_static_vars\",con)\n",
    "\n",
    "cur.close()\n",
    "con.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# write the data out to file\n",
    "df.set_index('icustay_id').to_csv('design_matrix_from_view.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(seed=72397)\n",
    "# move from a data frame into a numpy array\n",
    "X = df.values.astype(float)\n",
    "y = X[:,1].astype(float)\n",
    "\n",
    "icustay_id = X[:,0]\n",
    "\n",
    "# delete first 2 columns: the ID and the outcome\n",
    "X = np.delete(X,0,axis=1)\n",
    "X = np.delete(X,0,axis=1)\n",
    "\n",
    "# get a header row\n",
    "X_header = [xval for x, xval in enumerate(df.columns) if x > 1]\n",
    "\n",
    "X_orig = X\n",
    "X_header_orig = X_header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models = {'l2logreg': LogisticRegressionCV(penalty='l2',cv=5,fit_intercept=True),\n",
    "         'lasso': LassoCV(cv=5,fit_intercept=True),\n",
    "         'xgb': xgb.XGBClassifier(max_depth=3, n_estimators=300, learning_rate=0.05),\n",
    "         'logreg': LogisticRegression(fit_intercept=True)}\n",
    "\n",
    "scores = list()\n",
    "for i, mdl in enumerate(['logreg','l2logreg','lasso','xgb']):\n",
    "    if mdl == 'xgb':\n",
    "        # no pre-processing of data necessary\n",
    "        estimator = Pipeline([(mdl, models[mdl])])\n",
    "        \n",
    "    else:\n",
    "        estimator = Pipeline([(\"imputer\", Imputer(missing_values='NaN',\n",
    "                                          strategy=\"mean\",\n",
    "                                          axis=0)),\n",
    "                      (\"scaler\", StandardScaler()),\n",
    "                      (mdl, models[mdl])])\n",
    "    \n",
    "    scores.append(cross_val_score(estimator, X, y, scoring='roc_auc',cv=5))\n",
    "    \n",
    "    print('{:10s} - {:0.4f} [{:0.4f}, {:0.4f}]').format(mdl, np.mean(scores[i]), np.min(scores[i]), np.max(scores[i]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add in \"static\" variables - that is variables which do not change throughout a patient's stay (and arguably are not related to acuity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.merge(mp_static_vars, how='inner', on='icustay_id',suffixes=('','_static'))\n",
    "\n",
    "# move from a data frame into a numpy array\n",
    "X = df.values.astype(float)\n",
    "y = X[:,1].astype(float)\n",
    "\n",
    "# delete first 2 columns: the ID and the outcome\n",
    "X = np.delete(X,0,axis=1)\n",
    "X = np.delete(X,0,axis=1)\n",
    "\n",
    "# get a header row\n",
    "X_header = [xval for x, xval in enumerate(df.columns) if x > 1]\n",
    "\n",
    "models = {'l2logreg': LogisticRegressionCV(penalty='l2',cv=5,fit_intercept=True),\n",
    "         'lasso': LassoCV(cv=5,fit_intercept=True),\n",
    "         'xgb': xgb.XGBClassifier(max_depth=3, n_estimators=300, learning_rate=0.05),\n",
    "         'logreg': LogisticRegression(fit_intercept=True)}\n",
    "\n",
    "scores = list()\n",
    "for i, mdl in enumerate(['logreg','l2logreg','lasso','xgb']):\n",
    "    if mdl == 'xgb':\n",
    "        # no pre-processing of data necessary\n",
    "        estimator = Pipeline([(mdl, models[mdl])])\n",
    "        \n",
    "    else:\n",
    "        estimator = Pipeline([(\"imputer\", Imputer(missing_values='NaN',\n",
    "                                          strategy=\"mean\",\n",
    "                                          axis=0)),\n",
    "                      (\"scaler\", StandardScaler()),\n",
    "                      (mdl, models[mdl])])\n",
    "    \n",
    "    scores.append(cross_val_score(estimator, X, y, scoring='roc_auc',cv=5))\n",
    "    \n",
    "    print('{:10s} - {:0.4f} [{:0.4f}, {:0.4f}]').format(mdl, np.mean(scores[i]), np.min(scores[i]), np.max(scores[i]) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
