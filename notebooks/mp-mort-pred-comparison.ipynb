{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.font_manager import FontProperties # for unicode fonts\n",
    "import psycopg2\n",
    "import sys\n",
    "import datetime as dt\n",
    "import mp_utils as mp\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "# used to print out pretty pandas dataframes\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# used to impute mean for data and standardize for computational stability\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# logistic regression is our favourite model ever\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV # l2 regularized regression\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# used to calculate AUROC/accuracy\n",
    "from sklearn import metrics\n",
    "\n",
    "# used to create confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# gradient boosting - must download package https://github.com/dmlc/xgboost\n",
    "import xgboost as xgb\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# below config used on pc70\n",
    "sqluser = 'alistairewj'\n",
    "dbname = 'mimic'\n",
    "schema_name = 'mimiciii'\n",
    "query_schema = 'SET search_path to public,' + schema_name + ';'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "USE_SQL=0\n",
    "\n",
    "if USE_SQL:\n",
    "    # Connect to local postgres version of mimic\n",
    "    con = psycopg2.connect(dbname=dbname, user=sqluser)\n",
    "\n",
    "    # exclusion criteria:\n",
    "    #   - less than 15 years old\n",
    "    #   - stayed in the ICU less than 4 hours\n",
    "    #   - never have any chartevents data (i.e. likely administrative error)\n",
    "    #   - organ donor accounts (administrative \"readmissions\" for patients who died in hospital)\n",
    "    query = query_schema + \\\n",
    "    \"\"\"\n",
    "    select \n",
    "        *\n",
    "    from dm_cohort\n",
    "    where excluded = 0\n",
    "    \"\"\"\n",
    "    co = pd.read_sql_query(query,con)\n",
    "    \n",
    "    # convert the inclusion flags to boolean\n",
    "    for c in co.columns:\n",
    "        if c[0:10]=='inclusion_':\n",
    "            co[c] = co[c].astype(bool)\n",
    "\n",
    "    # extract static vars into a separate dataframe\n",
    "    df_static = pd.read_sql_query(query_schema + 'select * from mp_static_data', con)\n",
    "    #for dtvar in ['intime','outtime','deathtime']:\n",
    "    #    df_static[dtvar] = pd.to_datetime(df_static[dtvar])\n",
    "\n",
    "    vars_static = [u'is_male', u'emergency_admission', u'age',\n",
    "                   # services\n",
    "                   u'service_any_noncard_surg',\n",
    "                   u'service_any_card_surg',\n",
    "                   u'service_cmed',\n",
    "                   u'service_traum',\n",
    "                   u'service_nmed',\n",
    "                   # ethnicities\n",
    "                   u'race_black',u'race_hispanic',u'race_asian',u'race_other',\n",
    "                   # phatness\n",
    "                   u'height', u'weight', u'bmi']\n",
    "\n",
    "\n",
    "    # get ~5 million rows containing data from errbody\n",
    "    # this takes a little bit of time to load into memory (~2 minutes)\n",
    "\n",
    "    # %%time results\n",
    "    # CPU times: user 42.8 s, sys: 1min 3s, total: 1min 46s\n",
    "    # Wall time: 2min 7s\n",
    "\n",
    "    df = pd.read_sql_query(query_schema + 'select * from mp_data', con)\n",
    "    df.drop('subject_id',axis=1,inplace=True)\n",
    "    df.drop('hadm_id',axis=1,inplace=True)\n",
    "    df.sort_values(['icustay_id','hr'],axis=0,ascending=True,inplace=True)\n",
    "    print(df.shape)\n",
    "\n",
    "    # get death information\n",
    "    df_death = pd.read_sql_query(query_schema + \"\"\"\n",
    "    select \n",
    "    co.subject_id, co.hadm_id, co.icustay_id\n",
    "    , ceil(extract(epoch from (co.outtime - co.intime))/60.0/60.0) as dischtime_hours\n",
    "    , ceil(extract(epoch from (adm.deathtime - co.intime))/60.0/60.0) as deathtime_hours\n",
    "    , case when adm.deathtime is null then 0 else 1 end as death\n",
    "    from dm_cohort co\n",
    "    inner join admissions adm\n",
    "    on co.hadm_id = adm.hadm_id\n",
    "    where co.excluded = 0\n",
    "    \"\"\", con)\n",
    "    \n",
    "    # get censoring information\n",
    "    df_censor = pd.read_sql_query(query_schema + \"\"\"\n",
    "    select co.icustay_id, min(cs.charttime) as censortime\n",
    "    , ceil(extract(epoch from min(cs.charttime-co.intime) )/60.0/60.0) as censortime_hours\n",
    "    from dm_cohort co \n",
    "    inner join mp_code_status cs\n",
    "    on co.icustay_id = cs.icustay_id\n",
    "    where cmo+dnr+dni+dncpr+cmo_notes>0\n",
    "    and co.excluded = 0\n",
    "    group by co.icustay_id\n",
    "    \"\"\", con)\n",
    "\n",
    "    # get severity scores\n",
    "    df_soi = pd.read_sql_query(query_schema + \"\"\"\n",
    "    select \n",
    "    co.icustay_id\n",
    "    , case when adm.deathtime is null then 0 else 1 end as death\n",
    "    , sa.saps\n",
    "    , sa2.sapsii\n",
    "    , aps.apsiii\n",
    "    , so.sofa\n",
    "    , lo.lods\n",
    "    , oa.oasis\n",
    "    from dm_cohort co\n",
    "    inner join admissions adm\n",
    "    on co.hadm_id = adm.hadm_id\n",
    "    left join saps sa\n",
    "    on co.icustay_id = sa.icustay_id\n",
    "    left join sapsii sa2\n",
    "    on co.icustay_id = sa2.icustay_id\n",
    "    left join apsiii aps\n",
    "    on co.icustay_id = aps.icustay_id\n",
    "    left join sofa so\n",
    "    on co.icustay_id = so.icustay_id\n",
    "    left join lods lo\n",
    "    on co.icustay_id = lo.icustay_id\n",
    "    left join oasis oa\n",
    "    on co.icustay_id = oa.icustay_id\n",
    "    where co.excluded = 0\n",
    "    \"\"\", con)\n",
    "    \n",
    "    # extract static vars into a separate dataframe\n",
    "    df_static = pd.read_sql_query(query_schema + 'select * from mp_static_data', con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "USE_CSV=1\n",
    "\n",
    "if USE_CSV:\n",
    "    co = pd.read_csv('df_cohort.csv')\n",
    "    \n",
    "    # convert the inclusion flags to boolean\n",
    "    for c in co.columns:\n",
    "        if c[0:10]=='inclusion_':\n",
    "            co[c] = co[c].astype(bool)\n",
    "    df = pd.read_csv('df_data.csv')\n",
    "    df_static = pd.read_csv('df_static_data.csv')\n",
    "    df_censor = pd.read_csv('df_censor.csv')\n",
    "    df_death = pd.read_csv('df_death.csv')\n",
    "    df_soi = pd.read_csv('df_soi.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exclusion criteria\n",
    "\n",
    "Each study has its own exclusion criteria (sometimes studies have multiple experiments). We define a dictionary of all exclusions with the dictionary key as the study name. Some studies have multiple experiments, so we append *a*, *b*, or *c*.\n",
    "\n",
    "The dictionary stores a length 2 list. The first element defines the window for data extraction: it contains a dictionary of the windows and the corresponding window sizes. The second element is the exclusion criteria. Both are functions which use `co` or `df` as their input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# first we can define the different windows: there aren't that many!\n",
    "df_tmp=df_death.copy().merge(df_censor, how='left', left_on='icustay_id', right_on='icustay_id').set_index('icustay_id')\n",
    "\n",
    "# admission+12 hours\n",
    "time_12hr = df_tmp.copy()\n",
    "time_12hr['windowtime'] = 12\n",
    "time_12hr = time_12hr['windowtime'].to_dict()\n",
    "\n",
    "# admission+24 hours\n",
    "time_24hr = df_tmp.copy()\n",
    "time_24hr['windowtime'] = 24\n",
    "time_24hr = time_24hr['windowtime'].to_dict()\n",
    "\n",
    "# admission+48 hours\n",
    "time_48hr = df_tmp.copy()\n",
    "time_48hr['windowtime'] = 48\n",
    "time_48hr = time_48hr['windowtime'].to_dict()\n",
    "\n",
    "# admission+72 hours\n",
    "time_72hr = df_tmp.copy()\n",
    "time_72hr['windowtime'] = 72\n",
    "time_72hr = time_72hr['windowtime'].to_dict()\n",
    "\n",
    "# admission+96 hours\n",
    "time_96hr = df_tmp.copy()\n",
    "time_96hr['windowtime'] = 96\n",
    "time_96hr = time_96hr['windowtime'].to_dict()\n",
    "\n",
    "# entire stay\n",
    "time_all = df_tmp.copy()\n",
    "time_all = time_all['dischtime_hours'].apply(np.ceil).astype(int).to_dict()\n",
    "\n",
    "# 12 hours before the patient died/discharged\n",
    "time_predeath = df_tmp.copy()\n",
    "time_predeath['windowtime'] = time_predeath['dischtime_hours']\n",
    "idx = time_predeath['deathtime_hours']<time_predeath['dischtime_hours']\n",
    "time_predeath.loc[idx,'windowtime'] = time_predeath.loc[idx,'deathtime_hours']\n",
    "# move from discharge/death time to 12 hours beforehand\n",
    "time_predeath['windowtime'] = time_predeath['windowtime']-12\n",
    "time_predeath = time_predeath['windowtime'].apply(np.ceil).astype(int).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# example params used to extract patient data\n",
    "# element 1: dictionary specifying end time of window for each patient\n",
    "# element 2: size of window\n",
    "# element 3: extra hours added to make it easier to get data on labs (and allows us to get labs pre-ICU)\n",
    "# e.g. [time_24hr, 8, 24] is\n",
    "#   (1) window ends at admission+24hr\n",
    "#   (2) window is 8 hours long\n",
    "#   (3) lab window is 8+24=32 hours long\n",
    "\n",
    "# this one is used more than once, so we define it here\n",
    "hugExclFcn = lambda x: x.loc[x['inclusion_only_mimicii']&x['inclusion_over_18']&x['inclusion_hug2009_obs']&x['inclusion_hug2009_not_nsicu_csicu']&x['inclusion_first_admission']&x['inclusion_full_code']&x['inclusion_not_brain_death']&x['inclusion_not_crf']&x['inclusion_no_dialysis_first24hr'],'icustay_id'].values\n",
    "\n",
    "# physionet2012 subset - not exact but close\n",
    "def physChallExclFcn(x):\n",
    "    out = x.loc[x['inclusion_only_mimicii']&x['inclusion_over_18']&x['inclusion_stay_ge_48hr']&x['inclusion_has_saps'],'icustay_id'].values\n",
    "    out = np.sort(out)\n",
    "    out = out[0:4000]\n",
    "    return out\n",
    " \n",
    "# caballero2015 is a random subsample - then limits to 18yrs, resulting in 11648\n",
    "def caballeroExclFcn(x):\n",
    "    out = x.loc[x['inclusion_only_mimicii']&x['inclusion_over_18'],'icustay_id'].values\n",
    "    out = np.sort(out)\n",
    "    out = out[0:11648]\n",
    "    return out\n",
    "\n",
    "np.random.seed(546345)\n",
    "W_extra = 24\n",
    "\n",
    "exclusions = OrderedDict([\n",
    "['caballero2015dynamically_a',  [[time_24hr, 24, W_extra], caballeroExclFcn]],\n",
    "['caballero2015dynamically_b',  [[time_48hr, 48, W_extra], caballeroExclFcn]],\n",
    "['caballero2015dynamically_c',  [[time_72hr, 72, W_extra], caballeroExclFcn]],\n",
    "['calvert2016computational',    [[time_predeath, 5, W_extra], lambda x: x.loc[x['inclusion_over_18']&x['inclusion_calvert2016_obs']&x['inclusion_stay_ge_17hr']&x['inclusion_non_alc_icd9'],'icustay_id'].values]],\n",
    "['calvert2016using',            [[time_predeath, 5, W_extra], lambda x: x.loc[x['inclusion_over_18']&x['inclusion_calvert2016_obs']&x['inclusion_stay_ge_17hr']&x['inclusion_stay_le_500hr'],'icustay_id'].values]],\n",
    "['celi2012database_a',          [[time_72hr, 72, W_extra], lambda x: x.loc[x['inclusion_only_mimicii']&x['inclusion_over_18']&x['inclusion_aki_icd9'],'icustay_id'].values ]],\n",
    "['celi2012database_b',          [[time_24hr, 24, W_extra], lambda x: x.loc[x['inclusion_only_mimicii']&x['inclusion_over_18']&x['inclusion_sah_icd9'],'icustay_id'].values ]],\n",
    "['che2016recurrent_a',          [[time_48hr, 48, W_extra], lambda x: x.loc[x['inclusion_over_18'],'icustay_id'].values ]],\n",
    "['che2016recurrent_b',          [[time_48hr, 48, W_extra], physChallExclFcn ]],\n",
    "['ding2016mortality',           [[time_48hr, 48, W_extra], physChallExclFcn ]],\n",
    "['ghassemi2014unfolding_a',     [[time_24hr, 24, W_extra], lambda x: x.loc[x['inclusion_only_mimicii']&x['inclusion_over_18']&x['inclusion_ge_100_non_stop_words'],'icustay_id'].values]],\n",
    "['ghassemi2014unfolding_b',     [[time_12hr, 12, W_extra], lambda x: x.loc[x['inclusion_only_mimicii']&x['inclusion_over_18']&x['inclusion_ge_100_non_stop_words'],'icustay_id'].values]],\n",
    "['ghassemi2015multivariate',    [[time_24hr, 24, W_extra], lambda x: x.loc[x['inclusion_only_mimicii']&x['inclusion_over_18']&x['inclusion_ge_100_non_stop_words']&x['inclusion_gt_6_notes'],'icustay_id'].values]],\n",
    "['grnarova2016neural',          [[time_all,  24, W_extra], lambda x: x.loc[x['inclusion_over_18']&x['inclusion_multiple_hadm'],'icustay_id'].values]],\n",
    "['harutyunyan2017multitask',    [[time_48hr, 48, W_extra], lambda x: x.loc[x['inclusion_over_18']&x['inclusion_multiple_icustay'],'icustay_id'].values]],\n",
    "['hoogendoorn2016prediction',   [[time_24hr, 24, W_extra], lambda x: x.loc[x['inclusion_only_mimicii']&x['inclusion_over_18']&x['inclusion_hug2009_obs'],'icustay_id'].values]],\n",
    "['hug2009icu',                  [[time_24hr, 24, W_extra], hugExclFcn]],\n",
    "['johnson2012patient',          [[time_48hr, 48, W_extra], physChallExclFcn]],\n",
    "['johnson2014data',             [[time_48hr, 48, W_extra], physChallExclFcn]],\n",
    "['joshi2012prognostic',         [[time_24hr, 24, W_extra], hugExclFcn]],\n",
    "['joshi2016identifiable',       [[time_48hr, 48, W_extra], hugExclFcn]],\n",
    "['lee2015customization',        [[time_24hr, 24, W_extra], lambda x: x.loc[x['inclusion_only_mimicii']&x['inclusion_over_18']&x['inclusion_lee2015_service']&x['inclusion_has_saps'],'icustay_id'].values]],\n",
    "['lee2015personalized',         [[time_24hr, 24, W_extra], lambda x: x.loc[x['inclusion_only_mimicii']&x['inclusion_over_18']&x['inclusion_has_saps'],'icustay_id'].values]],\n",
    "['lee2017patient',              [[time_24hr, 24, W_extra], lambda x: x.loc[x['inclusion_only_mimicii']&x['inclusion_over_18']&x['inclusion_has_saps'],'icustay_id'].values]],\n",
    "['lehman2012risk',              [[time_24hr, 24, W_extra], lambda x: x.loc[x['inclusion_only_mimicii']&x['inclusion_over_18']&x['inclusion_has_saps']&x['inclusion_stay_ge_24hr']&x['inclusion_first_admission'],'icustay_id'].values]],\n",
    "['luo2016interpretable',        [[time_all,  24, W_extra], lambda x: x.loc[x['inclusion_only_mimicii']&x['inclusion_over_18']&x['inclusion_has_sapsii']&x['inclusion_no_disch_summary'],'icustay_id'].values]],\n",
    "['luo2016predicting',           [[time_24hr, 12, W_extra], lambda x: np.intersect1d(hugExclFcn(x),x.loc[x['inclusion_stay_ge_24hr'],'icustay_id'].values) ]],\n",
    "['pirracchio2015mortality',     [[time_24hr, 24, W_extra], lambda x: x.loc[x['inclusion_only_mimicii']&x['inclusion_over_15'],'icustay_id'].values ]],\n",
    "['ripoll2014sepsis',            [[time_24hr, 24, W_extra], lambda x: x.loc[x['inclusion_only_mimicii']&x['inclusion_over_18']&x['inclusion_has_saps']&x['inclusion_not_explicit_sepsis'],'icustay_id'].values]],\n",
    "['wojtusiak2017c',              [[time_all,  24, W_extra], lambda x: x.loc[x['inclusion_over_65']&x['inclusion_alive_hos_disch'],'icustay_id'].values]]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define var_static which is used later\n",
    "#TODO: should refactor so this isn't needed\n",
    "var_min, var_max, var_first, var_last, var_sum, var_first_early, var_last_early, var_static = mp.vars_of_interest()\n",
    "\n",
    "K=5\n",
    "np.random.seed(871)\n",
    "# get unique subject_id (this is needed later)\n",
    "sid = np.sort(np.unique(df_death['subject_id'].values))\n",
    "\n",
    "# assign k-fold\n",
    "idxK_sid = np.random.permutation(sid.shape[0])\n",
    "idxK_sid = np.mod(idxK_sid,K)\n",
    "\n",
    "# get indices which map subject_ids in sid to the X dataframe\n",
    "idxMap = np.searchsorted(sid, df_death['subject_id'].values)\n",
    "\n",
    "# use these indices to map the k-fold integers\n",
    "idxK = idxK_sid[idxMap]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mdl_val_all = dict()\n",
    "results_val_all = dict()\n",
    "pred_val_all = dict()\n",
    "tar_val_all = dict()\n",
    "\n",
    "# Rough timing info:\n",
    "#     rf - 3 seconds per fold\n",
    "#    xgb - 30 seconds per fold\n",
    "# logreg - 4 seconds per fold\n",
    "#  lasso - 8 seconds per fold\n",
    "models = OrderedDict([\n",
    "          ['xgb', xgb.XGBClassifier(max_depth=3, n_estimators=300, learning_rate=0.05)],\n",
    "          ['lasso', LassoCV(cv=5,fit_intercept=True,normalize=True,max_iter=10000)],\n",
    "          ['logreg', LogisticRegression(fit_intercept=True)],\n",
    "          ['rf', RandomForestClassifier()]\n",
    "         ])\n",
    "#models = OrderedDict([['xgb', xgb.XGBClassifier(max_depth=3, n_estimators=300, learning_rate=0.05)]])\n",
    "    \n",
    "with open('/data/Dropbox (MIT)/data_matters_results/results.txt','w') as fp:\n",
    "    fp.write('StudyName,SampleSize')\n",
    "    for mdl in models:\n",
    "        fp.write(',{}'.format(mdl))\n",
    "    fp.write('\\n')\n",
    "    \n",
    "for current_study in exclusions:    \n",
    "    print('\\n==================== {} =========='.format('='*len(current_study)))\n",
    "    print('========== BEGINNING {} =========='.format(current_study))\n",
    "    print('==================== {} =========='.format('='*len(current_study)))\n",
    "\n",
    "    params = exclusions[current_study][0]\n",
    "    df_data = mp.get_design_matrix(df, params[0], W=params[1], W_extra=params[2])\n",
    "\n",
    "    # get a list of icustay_id who stayed at least 12 hours\n",
    "    iid_keep = exclusions[current_study][1](co)\n",
    "    print('Reducing sample size from {} to {} ({:2.2f}%).'.format(\n",
    "            df_data.shape[0], iid_keep.shape[0], iid_keep.shape[0]*100.0 / df_data.shape[0]))\n",
    "    df_data = df_data.loc[iid_keep,:]\n",
    "    print('')\n",
    "\n",
    "    # load the data into a numpy array\n",
    "\n",
    "    # first, the data from static vars from df_static\n",
    "    X = df_data.merge(df_static.set_index('icustay_id')[var_static], how='left', left_index=True, right_index=True)\n",
    "    # next, add in the outcome: death in hospital\n",
    "    X = X.merge(df_death.set_index('icustay_id')[['death']], left_index=True, right_index=True)\n",
    "\n",
    "    # map above K-fold indices to this dataset\n",
    "    X = X.merge(df_death.set_index('icustay_id')[['subject_id']], left_index=True, right_index=True)\n",
    "    # get indices which map subject_ids in sid to the X dataframe\n",
    "    idxMap = np.searchsorted(sid, X['subject_id'].values)\n",
    "    # use these indices to map the k-fold integers\n",
    "    idxK = idxK_sid[idxMap]\n",
    "    # drop the subject_id column\n",
    "    X.drop('subject_id',axis=1,inplace=True)\n",
    "\n",
    "    # convert to numpy data (assumes target, death, is the last column)\n",
    "    X = X.values\n",
    "    y = X[:,-1]\n",
    "    X = X[:,0:-1]\n",
    "    X_header = [x for x in df_data.columns.values] + var_static\n",
    "    \n",
    "    mdl_val = dict()\n",
    "    results_val = dict()\n",
    "    pred_val = dict()\n",
    "    tar_val = dict()\n",
    "\n",
    "    for mdl in models:\n",
    "        print('=============== {} ==============='.format(mdl))\n",
    "        mdl_val[mdl] = list()\n",
    "        results_val[mdl] = list() # initialize list for scores\n",
    "        pred_val[mdl] = list()\n",
    "        tar_val[mdl] = list()\n",
    "\n",
    "        if mdl == 'xgb':\n",
    "            # no pre-processing of data necessary for xgb\n",
    "            estimator = Pipeline([(mdl, models[mdl])])\n",
    "\n",
    "        else:\n",
    "            estimator = Pipeline([(\"imputer\", Imputer(missing_values='NaN',\n",
    "                                              strategy=\"mean\",\n",
    "                                              axis=0)),\n",
    "                          (\"scaler\", StandardScaler()),\n",
    "                          (mdl, models[mdl])]) \n",
    "\n",
    "        for k in range(K):\n",
    "            # train the model using all but the kth fold\n",
    "            curr_mdl = estimator.fit(X[idxK != k, :],y[idxK != k])\n",
    "\n",
    "            # get prediction on this dataset\n",
    "            if mdl == 'lasso':\n",
    "                curr_prob = curr_mdl.predict(X[idxK == k, :])\n",
    "            else:\n",
    "                curr_prob = curr_mdl.predict_proba(X[idxK == k, :])\n",
    "                curr_prob = curr_prob[:,1]\n",
    "\n",
    "            pred_val[mdl].append(curr_prob)\n",
    "            tar_val[mdl].append(y[idxK == k])\n",
    "\n",
    "            # calculate score (AUROC)\n",
    "            curr_score = metrics.roc_auc_score(y[idxK == k], curr_prob)\n",
    "\n",
    "            # add score to list of scores\n",
    "            results_val[mdl].append(curr_score)\n",
    "\n",
    "            # save the current model\n",
    "            mdl_val[mdl].append(curr_mdl)\n",
    "\n",
    "            print('{} - Finished fold {} of {}. AUROC {:0.3f}.'.format(dt.datetime.now(), k+1, K, curr_score))\n",
    "\n",
    "    # create a pointer for above dicts with new var names\n",
    "    # we will likely re-use the dicts in subsequent calls for getting model perfomances\n",
    "    mdl_val_all[current_study] = mdl_val\n",
    "    results_val_all[current_study] = results_val\n",
    "    pred_val_all[current_study] = pred_val\n",
    "    tar_val_all[current_study] = tar_val\n",
    "    \n",
    "    # print to file\n",
    "    with open('/data/Dropbox (MIT)/data_matters_results/results.txt','a') as fp:\n",
    "        # print study name and sample size\n",
    "        fp.write( '{},{}'.format(current_study, X.shape[0] ) )\n",
    "        \n",
    "        for i, mdl in enumerate(models):\n",
    "            fp.write(',{:0.6f}'.format( np.mean(results_val[mdl]) ))\n",
    "        \n",
    "        fp.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== ============== ==========\n",
      "========== BEGINNING lehman2012risk ==========\n",
      "==================== ============== ==========\n",
      "Reducing sample size from 52050 to 21739 (41.77%).\n",
      "\n",
      "=============== xgb ===============\n",
      "2017-04-23 20:22:36.959390 - Finished fold 1 of 5. AUROC 0.877.\n",
      "2017-04-23 20:22:45.229326 - Finished fold 2 of 5. AUROC 0.883.\n",
      "2017-04-23 20:22:53.804337 - Finished fold 3 of 5. AUROC 0.891.\n",
      "2017-04-23 20:23:02.824029 - Finished fold 4 of 5. AUROC 0.891.\n",
      "2017-04-23 20:23:10.956716 - Finished fold 5 of 5. AUROC 0.897.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for /: 'dict' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-cfa130dc14cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmdl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m             \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m',{:0.4f}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_val\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/numpy/core/fromnumeric.pyc\u001b[0m in \u001b[0;36mmean\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m   2887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2888\u001b[0m     return _methods._mean(a, axis=axis, dtype=dtype,\n\u001b[0;32m-> 2889\u001b[0;31m                           out=out, **kwargs)\n\u001b[0m\u001b[1;32m   2890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2891\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/numpy/core/_methods.pyc\u001b[0m in \u001b[0;36m_mean\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mrcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mret\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mrcount\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for /: 'dict' and 'int'"
     ]
    }
   ],
   "source": [
    "EXAMPLE_RUN=True\n",
    "\n",
    "# Rough timing info:\n",
    "#     rf - 3 seconds per fold\n",
    "#    xgb - 30 seconds per fold\n",
    "# logreg - 4 seconds per fold\n",
    "#  lasso - 8 seconds per fold\n",
    "models = OrderedDict([\n",
    "          ['xgb', xgb.XGBClassifier(max_depth=3, n_estimators=300, learning_rate=0.05)],\n",
    "          ['lasso', LassoCV(cv=5,fit_intercept=True,normalize=True,max_iter=10000)],\n",
    "          ['logreg', LogisticRegression(fit_intercept=True)],\n",
    "          ['rf', RandomForestClassifier()]\n",
    "         ])\n",
    "#models = OrderedDict([['xgb', xgb.XGBClassifier(max_depth=3, n_estimators=300, learning_rate=0.05)]])\n",
    "    \n",
    "if EXAMPLE_RUN:\n",
    "    current_study = 'lehman2012risk'\n",
    "    \n",
    "    print('\\n==================== {} =========='.format('='*len(current_study)))\n",
    "    print('========== BEGINNING {} =========='.format(current_study))\n",
    "    print('==================== {} =========='.format('='*len(current_study)))\n",
    "\n",
    "    params = exclusions[current_study][0]\n",
    "    df_data = mp.get_design_matrix(df, params[0], W=params[1], W_extra=params[2])\n",
    "\n",
    "    # get a list of icustay_id who stayed at least 12 hours\n",
    "    iid_keep = exclusions['lehman2012risk'][1](co)\n",
    "    print('Reducing sample size from {} to {} ({:2.2f}%).'.format(\n",
    "            df_data.shape[0], iid_keep.shape[0], iid_keep.shape[0]*100.0 / df_data.shape[0]))\n",
    "    df_data = df_data.loc[iid_keep,:]\n",
    "    print('')\n",
    "\n",
    "    # load the data into a numpy array\n",
    "\n",
    "    # first, the data from static vars from df_static\n",
    "    X = df_data.merge(df_static.set_index('icustay_id')[var_static], how='left', left_index=True, right_index=True)\n",
    "    # next, add in the outcome: death in hospital\n",
    "    X = X.merge(df_death.set_index('icustay_id')[['death']], left_index=True, right_index=True)\n",
    "\n",
    "    # map above K-fold indices to this dataset\n",
    "    X = X.merge(df_death.set_index('icustay_id')[['subject_id']], left_index=True, right_index=True)\n",
    "    # get indices which map subject_ids in sid to the X dataframe\n",
    "    idxMap = np.searchsorted(sid, X['subject_id'].values)\n",
    "    # use these indices to map the k-fold integers\n",
    "    idxK = idxK_sid[idxMap]\n",
    "    # drop the subject_id column\n",
    "    X.drop('subject_id',axis=1,inplace=True)\n",
    "\n",
    "    # convert to numpy data (assumes target, death, is the last column)\n",
    "    X = X.values\n",
    "    y = X[:,-1]\n",
    "    X = X[:,0:-1]\n",
    "    X_header = [x for x in df_data.columns.values] + var_static\n",
    "    \n",
    "    mdl_val = dict()\n",
    "    results_val = dict()\n",
    "    pred_val = dict()\n",
    "    tar_val = dict()\n",
    "\n",
    "    for mdl in models:\n",
    "        print('=============== {} ==============='.format(mdl))\n",
    "        mdl_val[mdl] = list()\n",
    "        results_val[mdl] = list() # initialize list for scores\n",
    "        pred_val[mdl] = list()\n",
    "        tar_val[mdl] = list()\n",
    "\n",
    "        if mdl == 'xgb':\n",
    "            # no pre-processing of data necessary for xgb\n",
    "            estimator = Pipeline([(mdl, models[mdl])])\n",
    "\n",
    "        else:\n",
    "            estimator = Pipeline([(\"imputer\", Imputer(missing_values='NaN',\n",
    "                                              strategy=\"mean\",\n",
    "                                              axis=0)),\n",
    "                          (\"scaler\", StandardScaler()),\n",
    "                          (mdl, models[mdl])]) \n",
    "\n",
    "        for k in range(K):\n",
    "            # train the model using all but the kth fold\n",
    "            curr_mdl = estimator.fit(X[idxK != k, :],y[idxK != k])\n",
    "\n",
    "            # get prediction on this dataset\n",
    "            if mdl == 'lasso':\n",
    "                curr_prob = curr_mdl.predict(X[idxK == k, :])\n",
    "            else:\n",
    "                curr_prob = curr_mdl.predict_proba(X[idxK == k, :])\n",
    "                curr_prob = curr_prob[:,1]\n",
    "\n",
    "            pred_val[mdl].append(curr_prob)\n",
    "            tar_val[mdl].append(y[idxK == k])\n",
    "\n",
    "            # calculate score (AUROC)\n",
    "            curr_score = metrics.roc_auc_score(y[idxK == k], curr_prob)\n",
    "\n",
    "            # add score to list of scores\n",
    "            results_val[mdl].append(curr_score)\n",
    "\n",
    "            # save the current model\n",
    "            mdl_val[mdl].append(curr_mdl)\n",
    "\n",
    "            print('{} - Finished fold {} of {}. AUROC {:0.3f}.'.format(dt.datetime.now(), k+1, K, curr_score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
