{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import psycopg2\n",
    "import sys\n",
    "import datetime as dt\n",
    "import mp_utils as mp\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# used for train/test splits and cross validation\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# used to impute mean for data and standardize for computational stability\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# logistic regression is our favourite model ever\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV # l2 regularized regression\n",
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "# used to calculate AUROC/accuracy\n",
    "from sklearn import metrics\n",
    "\n",
    "# used to create confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "# gradient boosting - must download package https://github.com/dmlc/xgboost\n",
    "import xgboost as xgb\n",
    "\n",
    "# default colours for prettier plots\n",
    "col = [[0.9047, 0.1918, 0.1988],\n",
    "    [0.2941, 0.5447, 0.7494],\n",
    "    [0.3718, 0.7176, 0.3612],\n",
    "    [1.0000, 0.5482, 0.1000],\n",
    "    [0.4550, 0.4946, 0.4722],\n",
    "    [0.6859, 0.4035, 0.2412],\n",
    "    [0.9718, 0.5553, 0.7741],\n",
    "    [0.5313, 0.3359, 0.6523]];\n",
    "marker = ['v','o','d','^','s','o','+']\n",
    "ls = ['-','-','-','-','-','s','--','--']\n",
    "\n",
    "from __future__ import print_function "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# extract data from *all* patients to create init design matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# query path\n",
    "qpath = '/home/alistairewj/mimic-private-code/mortality-prediction/views-single-patient/'\n",
    "\n",
    "# below config used on pc70\n",
    "sqluser = 'alistairewj'\n",
    "dbname = 'mimic'\n",
    "schema_name = 'mimiciii'\n",
    "\n",
    "# Connect to local postgres version of mimic\n",
    "con = psycopg2.connect(dbname=dbname, user=sqluser)\n",
    "cur = con.cursor()\n",
    "cur.execute('SET search_path to ' + schema_name + ';')\n",
    "\n",
    "files = ['bloodgasarterial','gcs','height','labs',\n",
    "         'rass','rrt','rrt_range','uo','vasopressor','vent',\n",
    "         'vitals','weight']\n",
    "\n",
    "starttime = dt.datetime.now()\n",
    "currenttime = starttime\n",
    "print('Starting query extract - {}'.format(starttime))\n",
    "data = dict()\n",
    "for f in files:\n",
    "    query = 'select * from mpap_' + f #+ ' where icustay_id in (200003,200024)'\n",
    "    \n",
    "    # replace variable in python because using variables in SQL w/o psql is ridiculously complicated\n",
    "    data[f] = pd.read_sql_query(query,con)\n",
    "    \n",
    "    print('Finished {} - time elapsed: {}'.format(f, dt.datetime.now() - currenttime))\n",
    "    currenttime = dt.datetime.now()\n",
    "\n",
    "# extract static vars into a separate dataframe\n",
    "df_static = pd.read_sql_query('select * from mpap_static_vars',con)\n",
    "for dtvar in ['intime','outtime','deathtime']:\n",
    "    df_static[dtvar] = pd.to_datetime(df_static[dtvar])\n",
    "\n",
    "print('Finished all queries - time elapsed: {}'.format(dt.datetime.now() - starttime))\n",
    "cur.close()\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# change charttime into timeelapsed\n",
    "\n",
    "It's easier to work with seconds since admission, rather than an absolute charttime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# convert charttime to charttime_elapsed, starttime to starttime_elapsed, endtime to endtime_elapsed\n",
    "df_static_add = df_static[['icustay_id','intime']].set_index('icustay_id')\n",
    "\n",
    "\n",
    "for i, f in enumerate(data):\n",
    "    # convert charttime / starttime / endtime into elapsed times\n",
    "    # add in the intime to the dataframe\n",
    "    data[f] = data[f].merge(df_static_add,how='left',left_on='icustay_id',right_index=True)\n",
    "    \n",
    "    \n",
    "    # drop any missing ICUSTAY_ID or CHARTTIME from the frame\n",
    "    idxRem = data[f]['icustay_id'].isnull()\n",
    "    if np.sum(idxRem) > 0:\n",
    "        print('{:20s}... removing {} rows with null ICUSTAY_ID.'.format(f, np.sum(idxRem)))\n",
    "        data[f].drop(data[f].index[idxRem], axis=0, inplace=True)\n",
    "    \n",
    "    data[f]['icustay_id'] = data[f]['icustay_id'].astype(int)\n",
    "        \n",
    "        \n",
    "    if 'charttime' in data[f].columns:\n",
    "        idxRem = data[f]['charttime'].isnull()\n",
    "        if np.sum(idxRem) > 0:\n",
    "            print('{:20s}... removing {} rows with null CHARTTIME.'.format(f, np.sum(idxRem)))\n",
    "            data[f].drop(data[f].index[idxRem], axis=0, inplace=True)\n",
    "    \n",
    "        # convert charttime to timestamp if it was not recognized because of missing data\n",
    "        data[f]['charttime'] = pd.to_datetime(data[f]['charttime'])\n",
    "            \n",
    "            \n",
    "    if 'starttime' in data[f].columns:\n",
    "        if data[f].shape[0] == 0:\n",
    "            # empty array\n",
    "            data[f]['starttime_elapsed'] = np.zeros([0,])\n",
    "            data[f]['endtime_elapsed'] = np.zeros([0,])\n",
    "        else:\n",
    "            data[f]['starttime_elapsed'] = (data[f]['starttime'] - data[f]['intime']) / np.timedelta64(1, 's')\n",
    "            data[f]['endtime_elapsed'] = (data[f]['endtime'] - data[f]['intime']) / np.timedelta64(1, 's')\n",
    "        \n",
    "        data[f].drop('starttime', axis=1, inplace=True)\n",
    "        data[f].drop('endtime', axis=1, inplace=True)\n",
    "        \n",
    "    elif 'charttime' in data[f].columns:\n",
    "        if data[f].shape[0] == 0:\n",
    "            # empty array\n",
    "            data[f]['charttime_elapsed'] = np.zeros([0,])\n",
    "        else:\n",
    "            data[f]['charttime_elapsed'] =  (data[f]['charttime'] - data[f]['intime']) / np.timedelta64(1, 's')\n",
    "        \n",
    "        data[f].drop('charttime', axis=1, inplace=True)\n",
    "    \n",
    "    data[f].drop('intime', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = mp.collapse_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# now add in data where we have starttime/endtime, not just charttime\n",
    "\n",
    "\n",
    "# dictionary mapping table names to column name of interest\n",
    "colNameMap = {'vent': 'vent',\n",
    "              'vasopressor': 'vasopressor',\n",
    "              'rrt_range': 'rrt'}\n",
    "rangeTbl = ['vent','vasopressor','rrt_range']\n",
    "\n",
    "\n",
    "# initialize a dataframe with every possible time\n",
    "charttime = df[['icustay_id','charttime_elapsed']].copy()\n",
    "\n",
    "for f in rangeTbl:\n",
    "    df_tmp = data[f]\n",
    "    \n",
    "    colName = colNameMap[f]\n",
    "    \n",
    "    # initialize the data to all zeros\n",
    "    charttime.loc[:,colName] = np.zeros(charttime.shape[0])\n",
    "    \n",
    "    if df_tmp.shape[0] == 0:\n",
    "        # there is no data for this table - continue\n",
    "        continue\n",
    "    \n",
    "    # add in the values\n",
    "    df_tmp.loc[:,colName + '_start'] = np.ones(df_tmp.shape[0])\n",
    "    df_tmp.loc[:,colName + '_end'] = -1*np.ones(df_tmp.shape[0])\n",
    "    \n",
    "    # add the starttime/endtime as charttimes\n",
    "    # this ensures that we have a row in the master dataframe corresponding to the start/end time of these observations\n",
    "    charttime = pd.concat([charttime,\n",
    "                           df_tmp[['icustay_id','starttime_elapsed', colName + '_start']]\n",
    "                           .rename(columns = {'starttime_elapsed': 'charttime_elapsed', colName + '_start': colName})],\n",
    "                          ignore_index=True)\n",
    "    charttime = pd.concat( [charttime,\n",
    "                            df_tmp[['icustay_id','endtime_elapsed', colName + '_end']]\n",
    "                            .rename(columns = {'endtime_elapsed': 'charttime_elapsed', colName + '_end': colName})],\n",
    "                         ignore_index=True)\n",
    "\n",
    "charttime = charttime.drop_duplicates()\n",
    "\n",
    "# set indices for data\n",
    "df.set_index(['icustay_id','charttime_elapsed'],inplace=True)\n",
    "charttime.set_index(['icustay_id','charttime_elapsed'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create a variable which is 1 if the charttime is between starttime/endtime\n",
    "for f in rangeTbl:\n",
    "        \n",
    "    # set the value of each column @ the respective starttime to 1\n",
    "    colName = colNameMap[f]\n",
    "    \n",
    "    # impute 0 for NaNs now - this allows us to *subtract* 1 from the data\n",
    "    # when done in this order, we prevent errors in cumsum if starttime == endtime\n",
    "    charttime[colName].fillna(0,inplace=True)\n",
    "    \n",
    "# sort the data by the index\n",
    "charttime.sort_index(axis=0, ascending=True, inplace=True)\n",
    "\n",
    "for f in rangeTbl:\n",
    "    colName = colNameMap[f]\n",
    "    # cumulative sum each value\n",
    "    # this will assign all charttimes after starttime to 1\n",
    "    # the endtime will end up with a value of 0\n",
    "    charttime.loc[:,colName] = charttime.loc[:,colName].cumsum(axis=0, dtype=int, skipna=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# add in the charttime data to our full data\n",
    "# note that we set the values explicitly to avoid\n",
    "df = df.merge(charttime,\n",
    "                left_index=True, right_index=True,\n",
    "                suffixes=('','_range_variable'),\n",
    "                how='outer')\n",
    "\n",
    "# if the column already existed, a new column has been created, with suffix '_range_variable'\n",
    "# loop through these vars, and merge them with the original var\n",
    "# since they are binary flags, we can happily use max() to merge the data\n",
    "colFix = [x for x in df.columns if '_range_variable' in x]\n",
    "if len(colFix) > 0:\n",
    "    for f in colFix:\n",
    "        f_orig = f[:-15] # remove suffix\n",
    "\n",
    "        df.loc[:,f_orig] = df[[f_orig, f]].apply(max, axis=1)\n",
    "        df.drop(f,axis=1,inplace=True)\n",
    "    \n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# write the individual patient's data to file\n",
    "# this takes >30 minutes\n",
    "# each file is around 20-50kb\n",
    "for iid in df.index.levels[0]:\n",
    "    df_curr = df.loc[iid,:]\n",
    "    df_curr.to_csv('./data/' + str(int(iid)) + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataframe is now prepared. It contains timestamped observations for a number of features.\n",
    "\n",
    "The dataframe is completely denormalized, and is a bit inefficient space wise!\n",
    "\n",
    "Next, we extract all the design matrices we would like:\n",
    "    \n",
    "    * using a random offset, 4 hour window, somewhere between `INTIME` and (`OUTTIME - 4 hours`)\n",
    "    * using a random offset, 4 hour window, with offset for deaths fixed at DEATHTIME -  4 Hr\n",
    "    * using a random offset, 4 hour window, with offset for deaths fixed at DEATHTIME -  8 Hr\n",
    "    * using a random offset, 4 hour window, with offset for deaths fixed at DEATHTIME - 16 Hr\n",
    "    * using a random offset, 4 hour window, with offset for deaths fixed at DEATHTIME - 24 Hr\n",
    "    * using a fixed offset of hour 0, 4 hour window\n",
    "\n",
    "This involves:\n",
    "\n",
    "1. Setting a random seed\n",
    "2. Defining the start_dict dictionary (start times for each iid)\n",
    "3. Extracting/writing out the design matrix\n",
    "   \n",
    "Using the dictionary of starttimes (indicating when the window for data extraction should start), we extract a set of features. The features are usually first/last/min/max, and are defined in the utils subfunction (`vars_of_interest`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "analyses = ['00', '04', '08', '16', '24', # 'base', \n",
    "            'fixed', 'wt24_fixed', 'wt8', 'wt16', 'wt24']\n",
    "            #'wt8_00', 'wt8_08', 'wt8_16', 'wt8_24']\n",
    "\n",
    "seeds = {'base': 897234762,\n",
    "    'base_nodeathfix': 897234762,\n",
    "    '00': 85134,\n",
    "    '04': 64532,\n",
    "    '08': 10928,\n",
    "    '16': 98432,\n",
    "    '24': 98382,\n",
    "    'fixed': 48768,\n",
    "    'wt8': 743762,\n",
    "    'wt16': 81230,\n",
    "    'wt24': 809172,\n",
    "    'wt8_00': 45133,\n",
    "    'wt8_08': 13749,\n",
    "    'wt8_16': 85699,\n",
    "    'wt8_24': 34651,\n",
    "    'wt24_fixed': 19862}\n",
    "\n",
    "for i, a in enumerate(analyses):\n",
    "    # create a random offset for each patient's stay\n",
    "    np.random.seed(seed=seeds[a])\n",
    "    \n",
    "    # set the window length\n",
    "    if a in ['base','base_nodeathfix', 'fixed',\n",
    "            '00', '04', '08', '16', '24']:\n",
    "        T = 4\n",
    "        \n",
    "    elif a in ['wt8']:\n",
    "        T = 8\n",
    "        \n",
    "    elif a in ['wt16']:\n",
    "        T = 16\n",
    "        \n",
    "    elif a in ['wt24']:\n",
    "        T = 24\n",
    "    \n",
    "    elif 'wt8' in a and len(a)>3:\n",
    "        T = 8\n",
    "        T_before_death = int(a[-2:])\n",
    "        \n",
    "    # determine if we should fix the time for those who die\n",
    "    if a in ['00', '04', '08', '16', '24']:\n",
    "        T_before_death = int(a)\n",
    "    else:\n",
    "        T_before_death = None\n",
    "    \n",
    "    \n",
    "    # generate the start dictionary\n",
    "    if a == 'base_nodeathfix':\n",
    "        start_dict = mp.gen_random_offset(df_static, death_fix=False, T = T, T_before_death = T_before_death)\n",
    "        \n",
    "    elif 'fixed' in a:\n",
    "        df_tmp = df_static.copy()\n",
    "        df_tmp['zeros'] = 0\n",
    "        start_dict = df_tmp[['icustay_id','zeros']].set_index('icustay_id').to_dict()['zeros']\n",
    "        \n",
    "    else:\n",
    "        start_dict = mp.gen_random_offset(df_static, T = T, T_before_death = T_before_death)\n",
    "        \n",
    "        \n",
    "    file_ext = '_' + a\n",
    "    \n",
    "    # write the static data - with the randomly generated offsets - to file\n",
    "    df_static.set_index('icustay_id').to_csv('./icustays_offset' + file_ext + '.csv')\n",
    "\n",
    "    print('Beginning prepping data for {}: {}.'.format(a,dt.datetime.now()))\n",
    "    df_data = mp.extract_feature_ap(df, start_dict, offset = T*60.0)\n",
    "\n",
    "    # write the new data to csv\n",
    "    df_data.to_csv('design_matrix' + file_ext + '.csv',index_label='icustay_id')\n",
    "\n",
    "    print('Finished prepping data for {}: {}.'.format(a,dt.datetime.now()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
