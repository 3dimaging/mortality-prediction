{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model A\n",
    "\n",
    "Model A is trained using random time points for each patient. We first train this model, then evaluate it on a separate dataset with data extracted at fixed lead times from mortality for the patients who died in-hospital."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.font_manager import FontProperties # for unicode fonts\n",
    "import psycopg2\n",
    "import sys\n",
    "import datetime as dt\n",
    "import mp_utils as mp\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# used for train/test splits and cross validation\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# used to impute mean for data and standardize for computational stability\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# logistic regression is our favourite model ever\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV # l2 regularized regression\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# used to calculate AUROC/accuracy\n",
    "from sklearn import metrics\n",
    "\n",
    "# used to create confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "# gradient boosting - must download package https://github.com/dmlc/xgboost\n",
    "import xgboost as xgb\n",
    "\n",
    "# default colours for prettier plots\n",
    "col = [[0.9047, 0.1918, 0.1988],\n",
    "    [0.2941, 0.5447, 0.7494],\n",
    "    [0.3718, 0.7176, 0.3612],\n",
    "    [1.0000, 0.5482, 0.1000],\n",
    "    [0.4550, 0.4946, 0.4722],\n",
    "    [0.6859, 0.4035, 0.2412],\n",
    "    [0.9718, 0.5553, 0.7741],\n",
    "    [0.5313, 0.3359, 0.6523]];\n",
    "marker = ['v','o','d','^','s','o','+']\n",
    "ls = ['-','-','-','-','-','s','--','--']\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# below config used on pc70\n",
    "sqluser = 'alistairewj'\n",
    "dbname = 'mimic'\n",
    "schema_name = 'mimiciii'\n",
    "\n",
    "# Connect to local postgres version of mimic\n",
    "con = psycopg2.connect(dbname=dbname, user=sqluser)\n",
    "cur = con.cursor()\n",
    "cur.execute('SET search_path to ' + schema_name)\n",
    "\n",
    "# exclusion criteria:\n",
    "#   - less than 16 years old\n",
    "#   - stayed in the ICU less than 4 hours\n",
    "#   - never have any chartevents data (i.e. likely administrative error)\n",
    "query = \\\n",
    "\"\"\"\n",
    "with t1 as\n",
    "(\n",
    "select ie.icustay_id\n",
    "    , adm.HOSPITAL_EXPIRE_FLAG\n",
    "    , ROW_NUMBER() over (partition by ie.subject_id order by intime) as rn\n",
    "from icustays ie\n",
    "inner join admissions adm\n",
    "    on ie.hadm_id = adm.hadm_id\n",
    "inner join patients pat\n",
    "    on ie.subject_id = pat.subject_id\n",
    "    and ie.intime > (pat.dob + interval '16' year)\n",
    "where adm.HAS_CHARTEVENTS_DATA = 1\n",
    "and \n",
    "not (\n",
    "       (lower(diagnosis) like '%organ donor%' and deathtime is not null)\n",
    "    or (lower(diagnosis) like '%donor account%' and deathtime is not null)\n",
    "    )\n",
    "and (ie.outtime - ie.intime) >= interval '4' hour\n",
    ")\n",
    "select \n",
    "    icustay_id\n",
    "    , HOSPITAL_EXPIRE_FLAG\n",
    "from t1\n",
    "\"\"\"\n",
    "co = pd.read_sql_query(query,con)\n",
    "co.set_index('icustay_id',inplace=True)\n",
    "\n",
    "# extract static vars into a separate dataframe\n",
    "df_static = pd.read_sql_query('select * from mpap_static_vars',con)\n",
    "for dtvar in ['intime','outtime','deathtime']:\n",
    "    df_static[dtvar] = pd.to_datetime(df_static[dtvar])\n",
    "df_static.set_index('icustay_id',inplace=True)\n",
    "\n",
    "cur.close()\n",
    "con.close()\n",
    "\n",
    "vars_static = [u'male', u'emergency', u'age',\n",
    "               u'cmed', u'csurg', u'surg', u'nsurg',\n",
    "               u'surg_other', u'traum', u'nmed',\n",
    "               u'omed', u'ortho', u'gu', u'gyn', u'ent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seeds = {'base': 473010,\n",
    "    'base_nodeathfix': 217632,\n",
    "    '00': 724311,\n",
    "    '04': 952227,\n",
    "    '08': 721297,\n",
    "    '16': 968879,\n",
    "    '24': 608972,\n",
    "    'fixed': 585794,\n",
    "    'wt8': 176381,\n",
    "    'wt16': 658229,\n",
    "    'wt24': 635170,\n",
    "    'wt8_00': 34741,\n",
    "    'wt8_08': 95467,\n",
    "    'wt8_16': 85349,\n",
    "    'wt8_24': 89642}\n",
    "\n",
    "\n",
    "models = {'xgb': xgb.XGBClassifier(max_depth=3, n_estimators=300, learning_rate=0.05),\n",
    "          'lasso': LassoCV(cv=5,fit_intercept=True,normalize=True),\n",
    "          'logreg': LogisticRegression(fit_intercept=True),\n",
    "          'rf': RandomForestClassifier(),\n",
    "          #'svm': GridSearchCV(sklearn.svm.SVC(kernel='rbf',class_weight='balanced',probability=False),\n",
    "          #                   svm_parameters, cv=5, scoring='roc_auc')\n",
    "         }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: Using random time segments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above reported cross-validation performance in a variety of settings. We're also interested in *evaluating* the same model in the various settings. That is, training a model using random offsets, and then evaluating how it performs 4 hours before death, 8 hours, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reload(mp)\n",
    "path_for_data = ''\n",
    "analyses = ['base', '00', '04', '08', '16', '24']\n",
    "\n",
    "# extract the data used to train the model\n",
    "data_ext = 'base'\n",
    "np.random.seed(seed=seeds[data_ext])\n",
    "\n",
    "# load the data into a numpy array\n",
    "X, y, X_header = mp.load_design_matrix(co,\n",
    "                                       df_additional_data=df_static[vars_static],\n",
    "                                       data_ext=data_ext,\n",
    "                                      path=path_for_data)\n",
    "\n",
    "\n",
    "    \n",
    "# load into a dictionary the other various datasets/models\n",
    "X_val = dict()\n",
    "y_val = dict()\n",
    "X_header_val = dict()\n",
    "results_val = dict() # stores AUROCs across datasets\n",
    "mdl_val = dict() # stores the model trained across k-folds\n",
    "\n",
    "for i, data_ext in enumerate(analyses):\n",
    "\n",
    "    # load the data into a numpy array\n",
    "    X_val[data_ext], y_val[data_ext], X_header_val[data_ext] = mp.load_design_matrix(co,\n",
    "                                           df_additional_data=df_static[vars_static],\n",
    "                                           data_ext=data_ext, path=path_for_data)\n",
    "    results_val[data_ext] = dict()\n",
    "    \n",
    "print('{} - Finished loading data'.format(dt.datetime.now()))\n",
    "\n",
    "np.random.seed(seed=seeds[data_ext])\n",
    "\n",
    "# create k-fold indices\n",
    "K = 5 # number of folds\n",
    "idxK = np.random.permutation(X.shape[0])\n",
    "idxK = np.mod(idxK,K)\n",
    "\n",
    "for mdl in models:\n",
    "    print('=============== {} ==============='.format(mdl))\n",
    "    mdl_val[mdl] = list()\n",
    "\n",
    "\n",
    "    for data_ext in X_val:\n",
    "        results_val[data_ext][mdl] = list() # initialize list for scores\n",
    "\n",
    "    if mdl == 'xgb':\n",
    "        # no pre-processing of data necessary for xgb\n",
    "        estimator = Pipeline([(mdl, models[mdl])])\n",
    "\n",
    "    else:\n",
    "        estimator = Pipeline([(\"imputer\", Imputer(missing_values='NaN',\n",
    "                                          strategy=\"mean\",\n",
    "                                          axis=0)),\n",
    "                      (\"scaler\", StandardScaler()),\n",
    "                      (mdl, models[mdl])]) \n",
    "\n",
    "    for k in range(K):\n",
    "        # train the model using all but the kth fold\n",
    "        curr_mdl = estimator.fit(X[idxK != k, :],y[idxK != k])\n",
    "\n",
    "        for data_ext in X_val:\n",
    "            # get prediction on this dataset\n",
    "            if mdl == 'lasso':\n",
    "                curr_mdl.predict(X_val[data_ext][idxK == k, :])\n",
    "            else:\n",
    "                curr_prob = curr_mdl.predict_proba(X_val[data_ext][idxK == k, :])\n",
    "                curr_prob = curr_prob[:,1]\n",
    "\n",
    "            # calculate score (AUROC)\n",
    "            curr_score = metrics.roc_auc_score(y_val[data_ext][idxK == k], curr_prob)\n",
    "\n",
    "            # add score to list of scores\n",
    "            results_val[data_ext][mdl].append(curr_score)\n",
    "\n",
    "            # save the current model\n",
    "            mdl_val[mdl].append(curr_mdl)\n",
    "\n",
    "        print('{} - Finished fold {} of {}.'.format(dt.datetime.now(), k+1, K))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('=============== {} ==============='.format(mdl))\n",
    "mdl_val[mdl] = list()\n",
    "\n",
    "\n",
    "for data_ext in X_val:\n",
    "    results_val[data_ext][mdl] = list() # initialize list for scores\n",
    "\n",
    "if mdl == 'xgb':\n",
    "    # no pre-processing of data necessary for xgb\n",
    "    estimator = Pipeline([(mdl, models[mdl])])\n",
    "\n",
    "else:\n",
    "    estimator = Pipeline([(\"imputer\", Imputer(missing_values='NaN',\n",
    "                                      strategy=\"mean\",\n",
    "                                      axis=0)),\n",
    "                  (\"scaler\", StandardScaler()),\n",
    "                  (mdl, models[mdl])]) \n",
    "\n",
    "for k in range(K):\n",
    "    # train the model using all but the kth fold\n",
    "    curr_mdl = estimator.fit(X[idxK != k, :],y[idxK != k])\n",
    "\n",
    "    for data_ext in X_val:\n",
    "        # get prediction on this dataset\n",
    "        if mdl == 'lasso':\n",
    "            curr_mdl.predict(X_val[data_ext][idxK == k, :])\n",
    "        else:\n",
    "            curr_prob = curr_mdl.predict_proba(X_val[data_ext][idxK == k, :])\n",
    "            curr_prob = curr_prob[:,1]\n",
    "\n",
    "        # calculate score (AUROC)\n",
    "        curr_score = metrics.roc_auc_score(y_val[data_ext][idxK == k], curr_prob)\n",
    "\n",
    "        # add score to list of scores\n",
    "        results_val[data_ext][mdl].append(curr_score)\n",
    "\n",
    "        # save the current model\n",
    "        mdl_val[mdl].append(curr_mdl)\n",
    "\n",
    "    print('{} - Finished fold {} of {}.'.format(dt.datetime.now(), k+1, K))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results_val[data_ext]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pretty_labels = {'xgb': 'GB', 'rf': 'RF', 'logreg': 'LR'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot a figure of the results\n",
    "xi_str = ['00','04','08','16','24']\n",
    "xi = [int(x) for x in xi_str]\n",
    "\n",
    "plt.figure(figsize=[8,5])\n",
    "\n",
    "for m, mdl in enumerate(['rf','xgb','logreg']):\n",
    "    all_score = list()\n",
    "    for i, x in enumerate(xi_str):\n",
    "        curr_score = results_val[x][mdl]\n",
    "\n",
    "        plt.plot(int(x) * np.ones(len(curr_score)), curr_score,\n",
    "                marker=marker[m], color=col[m],\n",
    "                markersize=10, linewidth=2, linestyle=':')\n",
    "\n",
    "        all_score.append(np.median(curr_score))\n",
    "        \n",
    "    # plot a line through the mean across all evaluations\n",
    "\n",
    "    plt.plot(xi, all_score,\n",
    "            marker=marker[m], color=col[m],\n",
    "            markersize=10, linewidth=2, linestyle='-',\n",
    "            label=pretty_labels[mdl])\n",
    "\n",
    "plt.gca().set_xticks(np.linspace(0,24,7))\n",
    "plt.gca().set_xlim([-1,25])\n",
    "plt.gca().invert_xaxis()\n",
    "plt.xlabel('Lead time (hours)',fontsize=18)\n",
    "plt.ylabel('AUROC',fontsize=18)\n",
    "plt.legend(loc='lower left',fontsize=16)\n",
    "\n",
    "ax = plt.gca()\n",
    "\n",
    "for tick in ax.xaxis.get_major_ticks():\n",
    "    tick.label.set_fontsize(16) \n",
    "for tick in ax.yaxis.get_major_ticks():\n",
    "    tick.label.set_fontsize(16) \n",
    "    \n",
    "plt.grid()\n",
    "plt.savefig('auroc_over_time.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat the same experiment as above, but this time, let's train a model with the outcome \"did the patient die in the next 24 hours?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# extract the data\n",
    "data_ext = 'base'\n",
    "\n",
    "# load the data into a numpy array\n",
    "X, y, X_header = mp.load_design_matrix(co,\n",
    "                                       df_additional_data=df_static[vars_static],\n",
    "                                       data_ext=data_ext,\n",
    "                                       diedWithin=24, path=path_for_data)\n",
    "\n",
    "# load into a dictionary the other various datasets/models\n",
    "X_val = dict()\n",
    "y_val = dict()\n",
    "X_header_val = dict()\n",
    "results_val_dw24 = dict() # stores AUROCs across datasets\n",
    "mdl_val_dw24 = dict() # stores the model trained across k-folds\n",
    "\n",
    "for i, data_ext in enumerate(analyses):\n",
    "\n",
    "    # load the data into a numpy array\n",
    "    X_val[data_ext], y_val[data_ext], X_header_val[data_ext] = mp.load_design_matrix(co,\n",
    "                                           df_additional_data=df_static[vars_static],\n",
    "                                           data_ext='_' + data_ext, path=path_for_data)\n",
    "    results_val_dw24[data_ext] = dict()\n",
    "    \n",
    "print('{} - Finished loading data'.format(dt.datetime.now()))\n",
    "\n",
    "np.random.seed(seed=seeds[data_ext])\n",
    "# create k-fold indices\n",
    "K = 5 # number of folds\n",
    "idxK = np.random.permutation(X.shape[0])\n",
    "idxK = np.mod(idxK,K)\n",
    "\n",
    "\n",
    "for mdl in models:\n",
    "    print('=============== {} ==============='.format(mdl))\n",
    "    mdl_val_dw24[mdl] = list()\n",
    "\n",
    "\n",
    "    for data_ext in X_val:\n",
    "        results_val_dw24[data_ext][mdl] = list() # initialize list for scores\n",
    "\n",
    "    if mdl == 'xgb':\n",
    "        # no pre-processing of data necessary for xgb\n",
    "        estimator = Pipeline([(mdl, models[mdl])])\n",
    "\n",
    "    else:\n",
    "        estimator = Pipeline([(\"imputer\", Imputer(missing_values='NaN',\n",
    "                                          strategy=\"mean\",\n",
    "                                          axis=0)),\n",
    "                      (\"scaler\", StandardScaler()),\n",
    "                      (mdl, models[mdl])]) \n",
    "\n",
    "    for k in range(K):\n",
    "        # train the model using all but the kth fold\n",
    "        curr_mdl = estimator.fit(X[idxK != k, :],y[idxK != k])\n",
    "\n",
    "        for data_ext in X_val:\n",
    "            # get prediction on this dataset\n",
    "            if mdl == 'lasso':\n",
    "                curr_mdl.predict(X_val[data_ext][idxK == k, :])\n",
    "            else:\n",
    "                curr_prob = curr_mdl.predict_proba(X_val[data_ext][idxK == k, :])\n",
    "                curr_prob = curr_prob[:,1]\n",
    "\n",
    "            # calculate score (AUROC)\n",
    "            curr_score = metrics.roc_auc_score(y_val[data_ext][idxK == k], curr_prob)\n",
    "\n",
    "            # add score to list of scores\n",
    "            results_val_dw24[data_ext][mdl].append(curr_score)\n",
    "\n",
    "            # save the current model\n",
    "            mdl_val_dw24[mdl].append(curr_mdl)\n",
    "\n",
    "        print('{} - Finished fold {} of {}.'.format(dt.datetime.now(), k+1, K))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot a figure of the results\n",
    "xi_str = ['00','04','08','16','24']\n",
    "xi = [int(x) for x in xi_str]\n",
    "\n",
    "plt.figure(figsize=[8,6])\n",
    "\n",
    "for m, mdl in enumerate(['rf','xgb','logreg']):\n",
    "    all_score = list()\n",
    "    for i, x in enumerate(xi_str):\n",
    "        curr_score = results_val_dw24[x][mdl]\n",
    "\n",
    "        plt.plot(int(x) * np.ones(len(curr_score)), curr_score,\n",
    "                marker=marker[m], color=col[m],\n",
    "                markersize=10, linewidth=2, linestyle=':')\n",
    "\n",
    "        all_score.append(np.median(curr_score))\n",
    "        \n",
    "    # plot a line through the mean across all evaluations\n",
    "\n",
    "    plt.plot(xi, all_score,\n",
    "            marker=marker[m], color=col[m],\n",
    "            markersize=10, linewidth=2, linestyle='-',\n",
    "            label=pretty_labels[mdl])\n",
    "\n",
    "plt.gca().set_xticks(np.linspace(0,24,7))\n",
    "plt.gca().set_xlim([-1,25])\n",
    "plt.gca().invert_xaxis()\n",
    "plt.legend(loc='lower center',fontsize=16)\n",
    "plt.xlabel('Lead time (hours)',fontsize=18)\n",
    "plt.ylabel('AUROC',fontsize=18)\n",
    "\n",
    "ax = plt.gca()\n",
    "\n",
    "for tick in ax.xaxis.get_major_ticks():\n",
    "    tick.label.set_fontsize(16) \n",
    "for tick in ax.yaxis.get_major_ticks():\n",
    "    tick.label.set_fontsize(16) \n",
    "\n",
    "plt.grid()\n",
    "plt.savefig('auroc_over_time_dw24.pdf')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
